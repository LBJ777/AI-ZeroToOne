{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Study\n",
    "## Two main features of PyTorch\n",
    " - Tensor: similar to numpy.array but can run on GPUs\n",
    " - Autograd: automatic differentiation for all operations on Tensors\n",
    "\n",
    "## Materials\n",
    " - [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    " - [PyTorch Examples](https://github.com/jcjohnson/pytorch-examples)\n",
    "\n",
    "## Examples\n",
    " - 3 Layers Neural Network\n",
    "   - Input Layer: 1000 neurons\n",
    "   - Hidden Layer: 100 neurons\n",
    "      - ReLU Activation Function\n",
    "   - Output Layer: 10 neurons\n",
    "- Training Data: 100 samples\n",
    "   - Learning Rate: 1e-6\n",
    "   - Training Iterations: 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2809927.4984153016\n",
      "1 2895192.164057741\n",
      "2 9195337.62595426\n",
      "3 52672674.94903434\n",
      "4 305856609.5553557\n",
      "5 1045698768.0498496\n",
      "6 199561235.26916894\n",
      "7 25670508.36144746\n",
      "8 14668513.15366512\n",
      "9 9938381.689097475\n",
      "10 7305750.080500455\n",
      "11 5639376.168415225\n",
      "12 4516688.973984517\n",
      "13 3713291.893420212\n",
      "14 3109781.9489851957\n",
      "15 2646019.473735994\n",
      "16 2279626.8021711316\n",
      "17 1983933.162397087\n",
      "18 1742160.4876639124\n",
      "19 1542133.7768047203\n",
      "20 1374561.2308388902\n",
      "21 1233790.2257218708\n",
      "22 1113270.240378296\n",
      "23 1008865.8361852948\n",
      "24 918241.0627890414\n",
      "25 840077.572144227\n",
      "26 771346.8382308907\n",
      "27 710486.5453011404\n",
      "28 656157.9888915124\n",
      "29 607437.4684917162\n",
      "30 563553.4739288153\n",
      "31 523936.1619861584\n",
      "32 487896.0999278911\n",
      "33 455015.8810166172\n",
      "34 424950.28932338505\n",
      "35 397828.61352782807\n",
      "36 373114.01286175643\n",
      "37 350586.7383271787\n",
      "38 330076.21784706844\n",
      "39 311372.4925737009\n",
      "40 294105.3357829785\n",
      "41 278128.95856948197\n",
      "42 263351.43019166414\n",
      "43 249582.38174369294\n",
      "44 236842.7945321482\n",
      "45 225021.53143765056\n",
      "46 214055.88220353192\n",
      "47 203861.97800410207\n",
      "48 194417.3138544439\n",
      "49 185606.050166901\n",
      "50 177394.0409903812\n",
      "51 169683.58821358642\n",
      "52 162459.27880610066\n",
      "53 155670.17782930526\n",
      "54 149274.28814720124\n",
      "55 143231.71605223024\n",
      "56 137534.86230123477\n",
      "57 132147.5728995408\n",
      "58 127041.64562164276\n",
      "59 122185.20835987577\n",
      "60 117598.82371798554\n",
      "61 113244.90867794665\n",
      "62 109106.01596621695\n",
      "63 105164.84071554855\n",
      "64 101416.6164703711\n",
      "65 97847.47812078249\n",
      "66 94470.93414405957\n",
      "67 91271.13840856109\n",
      "68 88249.67676456958\n",
      "69 85389.2087666667\n",
      "70 82668.72146635703\n",
      "71 80069.02473607403\n",
      "72 77590.43933780438\n",
      "73 75235.46241535955\n",
      "74 72984.77264337776\n",
      "75 70822.01044556877\n",
      "76 68747.58910486022\n",
      "77 66756.78520241655\n",
      "78 64852.33177184533\n",
      "79 63028.68106359597\n",
      "80 61272.65038432165\n",
      "81 59585.83493129232\n",
      "82 57964.99519447371\n",
      "83 56416.95783309253\n",
      "84 54928.278130331215\n",
      "85 53491.14640056565\n",
      "86 52104.633051631274\n",
      "87 50772.915217999514\n",
      "88 49488.52147690244\n",
      "89 48255.808588642096\n",
      "90 47067.408689473785\n",
      "91 45918.79542083961\n",
      "92 44806.476791572146\n",
      "93 43735.59557916774\n",
      "94 42698.68844246799\n",
      "95 41702.77421224156\n",
      "96 40738.742716452136\n",
      "97 39810.52830905516\n",
      "98 38911.37414001239\n",
      "99 38039.968707908294\n",
      "100 37194.61079869843\n",
      "101 36379.880620921904\n",
      "102 35591.47241170816\n",
      "103 34826.86432103477\n",
      "104 34087.33747840144\n",
      "105 33370.58016657945\n",
      "106 32674.5189550835\n",
      "107 31998.85396053106\n",
      "108 31343.695907466266\n",
      "109 30710.57909787457\n",
      "110 30095.78438437633\n",
      "111 29496.828626201273\n",
      "112 28917.423067458334\n",
      "113 28356.029613312767\n",
      "114 27808.123660145055\n",
      "115 27275.83923047134\n",
      "116 26761.07141984118\n",
      "117 26259.776142282306\n",
      "118 25771.09728257958\n",
      "119 25293.970644392528\n",
      "120 24829.300347976823\n",
      "121 24378.08008621713\n",
      "122 23937.709338108132\n",
      "123 23508.116323105558\n",
      "124 23090.682344887744\n",
      "125 22684.027430910257\n",
      "126 22289.97658709209\n",
      "127 21906.83879759453\n",
      "128 21533.06617198403\n",
      "129 21169.44853528614\n",
      "130 20814.601984880726\n",
      "131 20467.331978077436\n",
      "132 20127.26427129031\n",
      "133 19795.407273788875\n",
      "134 19470.797938097883\n",
      "135 19153.43351978595\n",
      "136 18842.701348757568\n",
      "137 18541.452721558067\n",
      "138 18247.803988111325\n",
      "139 17960.719701027232\n",
      "140 17679.72381084992\n",
      "141 17406.913975747855\n",
      "142 17139.388109470383\n",
      "143 16877.061781111905\n",
      "144 16620.706446868204\n",
      "145 16370.113059045727\n",
      "146 16124.514450294237\n",
      "147 15885.977228797165\n",
      "148 15651.881394922479\n",
      "149 15422.251983982651\n",
      "150 15197.554943287134\n",
      "151 14977.861537993067\n",
      "152 14762.094160739547\n",
      "153 14550.842999967976\n",
      "154 14344.146771964284\n",
      "155 14141.718786623234\n",
      "156 13942.648599598506\n",
      "157 13747.355033343054\n",
      "158 13555.965640920645\n",
      "159 13368.527385480289\n",
      "160 13183.297692733984\n",
      "161 13001.726200699313\n",
      "162 12825.20074914924\n",
      "163 12652.465694648115\n",
      "164 12482.729613593345\n",
      "165 12316.230324094602\n",
      "166 12152.62389349091\n",
      "167 11992.135600014773\n",
      "168 11834.066811203851\n",
      "169 11678.823289833457\n",
      "170 11526.505211676134\n",
      "171 11376.842397840655\n",
      "172 11229.758502577277\n",
      "173 11085.313665882855\n",
      "174 10943.183250268525\n",
      "175 10803.234059856075\n",
      "176 10665.74299352701\n",
      "177 10530.761203759057\n",
      "178 10398.397577101474\n",
      "179 10268.962184144953\n",
      "180 10141.35826393657\n",
      "181 10015.966370941062\n",
      "182 9892.750570875549\n",
      "183 9771.492061849533\n",
      "184 9652.223674942717\n",
      "185 9534.742791710143\n",
      "186 9418.986979139945\n",
      "187 9305.275573338555\n",
      "188 9193.212893996986\n",
      "189 9083.027518493182\n",
      "190 8974.951012531981\n",
      "191 8868.348684002553\n",
      "192 8763.39467764779\n",
      "193 8659.965008220699\n",
      "194 8558.069129738527\n",
      "195 8457.69327097293\n",
      "196 8359.08754005879\n",
      "197 8262.108266828098\n",
      "198 8166.544724302314\n",
      "199 8072.286863768929\n",
      "200 7979.485368636013\n",
      "201 7887.892637383313\n",
      "202 7797.890396471488\n",
      "203 7709.19360967961\n",
      "204 7621.7861256751585\n",
      "205 7535.558523576791\n",
      "206 7450.705897944674\n",
      "207 7367.072019632968\n",
      "208 7284.728896828063\n",
      "209 7203.460471221628\n",
      "210 7123.382024863597\n",
      "211 7044.452356110722\n",
      "212 6966.818522085795\n",
      "213 6890.34588497588\n",
      "214 6815.1102715299485\n",
      "215 6740.616960457865\n",
      "216 6667.338037832989\n",
      "217 6595.093633511841\n",
      "218 6523.835771145715\n",
      "219 6453.528769429879\n",
      "220 6385.144163258894\n",
      "221 6317.768671299695\n",
      "222 6251.352408847472\n",
      "223 6185.797803698152\n",
      "224 6121.237423957089\n",
      "225 6057.545980630141\n",
      "226 5994.830468718219\n",
      "227 5932.932255852445\n",
      "228 5871.756512654878\n",
      "229 5811.427293769966\n",
      "230 5751.907733947193\n",
      "231 5693.251942801785\n",
      "232 5635.358696133559\n",
      "233 5578.238192288774\n",
      "234 5521.865235186575\n",
      "235 5466.670713652012\n",
      "236 5412.335843698085\n",
      "237 5358.59277812136\n",
      "238 5305.53330932852\n",
      "239 5253.140573258493\n",
      "240 5201.426305895817\n",
      "241 5150.3189180610425\n",
      "242 5099.835106763831\n",
      "243 5050.061627044948\n",
      "244 5001.26557193961\n",
      "245 4953.858012144\n",
      "246 4907.0345753584\n",
      "247 4860.790642775711\n",
      "248 4815.059753928097\n",
      "249 4769.860977315117\n",
      "250 4725.564288547082\n",
      "251 4682.076361901141\n",
      "252 4639.07939709471\n",
      "253 4596.539446270872\n",
      "254 4554.540300388065\n",
      "255 4512.979624252323\n",
      "256 4472.09657533932\n",
      "257 4432.067100475454\n",
      "258 4392.6803123819855\n",
      "259 4353.522837680401\n",
      "260 4314.842433090991\n",
      "261 4276.598861680459\n",
      "262 4238.988925428061\n",
      "263 4201.828532293958\n",
      "264 4165.1749222585195\n",
      "265 4128.978837929379\n",
      "266 4093.1690320945054\n",
      "267 4057.764675474279\n",
      "268 4022.7412840347647\n",
      "269 3988.204287263209\n",
      "270 3954.104388104267\n",
      "271 3920.353859323827\n",
      "272 3886.935851708323\n",
      "273 3853.8897384906695\n",
      "274 3821.259255628055\n",
      "275 3788.9825443476757\n",
      "276 3757.1011812029747\n",
      "277 3725.5974208931325\n",
      "278 3694.494133607224\n",
      "279 3663.701136968503\n",
      "280 3633.220742889327\n",
      "281 3603.1553631104466\n",
      "282 3573.1338155129542\n",
      "283 3543.425257034604\n",
      "284 3514.164884793812\n",
      "285 3485.157062980315\n",
      "286 3456.461615473053\n",
      "287 3428.08802308685\n",
      "288 3400.000928649789\n",
      "289 3372.198602046032\n",
      "290 3344.7201103610882\n",
      "291 3317.5725594235164\n",
      "292 3290.674576400778\n",
      "293 3264.139360194377\n",
      "294 3237.8716061477508\n",
      "295 3211.89011728123\n",
      "296 3186.1813146605564\n",
      "297 3160.7580491550457\n",
      "298 3135.580297191732\n",
      "299 3110.6603928005034\n",
      "300 3086.0121462927514\n",
      "301 3061.6494534065237\n",
      "302 3037.522327190141\n",
      "303 3013.66899640905\n",
      "304 2990.071868351132\n",
      "305 2966.8330986252895\n",
      "306 2943.7457439559967\n",
      "307 2920.8363035598068\n",
      "308 2898.1914170897853\n",
      "309 2875.762607867574\n",
      "310 2853.574958426301\n",
      "311 2831.6227627999074\n",
      "312 2810.0965208808925\n",
      "313 2788.724574486052\n",
      "314 2767.567857809821\n",
      "315 2746.6461607603524\n",
      "316 2725.986864413279\n",
      "317 2705.5296046713493\n",
      "318 2685.255754194698\n",
      "319 2665.215234889197\n",
      "320 2645.3247536179865\n",
      "321 2625.6392888238265\n",
      "322 2606.154949477184\n",
      "323 2586.8610557708244\n",
      "324 2567.786601887377\n",
      "325 2548.88627661808\n",
      "326 2530.25906507076\n",
      "327 2511.7393419621094\n",
      "328 2493.374665830671\n",
      "329 2475.263522187552\n",
      "330 2457.3078999427034\n",
      "331 2439.5545339142086\n",
      "332 2421.9634816892158\n",
      "333 2404.5302906253314\n",
      "334 2387.290937643526\n",
      "335 2370.1751041225693\n",
      "336 2353.4280479276836\n",
      "337 2336.7763754930097\n",
      "338 2320.385861364635\n",
      "339 2304.1684071682103\n",
      "340 2288.136913665795\n",
      "341 2272.3166984948502\n",
      "342 2256.6197316289604\n",
      "343 2241.0425603298645\n",
      "344 2225.5802886820175\n",
      "345 2210.2747709446785\n",
      "346 2195.1322578245818\n",
      "347 2180.1140879377276\n",
      "348 2165.26477931751\n",
      "349 2150.552442688675\n",
      "350 2135.942172673413\n",
      "351 2121.474775540865\n",
      "352 2107.1410800403123\n",
      "353 2092.944182256938\n",
      "354 2078.926377505461\n",
      "355 2065.016069314795\n",
      "356 2051.228983806458\n",
      "357 2037.552033165759\n",
      "358 2024.0286111305709\n",
      "359 2010.604063564637\n",
      "360 1997.3052739560385\n",
      "361 1984.140070590604\n",
      "362 1971.085367281283\n",
      "363 1958.161442117471\n",
      "364 1945.3582905504404\n",
      "365 1932.6595996106853\n",
      "366 1920.0855555295916\n",
      "367 1907.641921601643\n",
      "368 1895.2781246276325\n",
      "369 1883.017498986416\n",
      "370 1870.8680149195964\n",
      "371 1858.8523743190935\n",
      "372 1846.9204305984913\n",
      "373 1835.1152982434676\n",
      "374 1823.4136074711137\n",
      "375 1811.8273019585597\n",
      "376 1800.3157675729906\n",
      "377 1788.8932027603096\n",
      "378 1777.5727461288075\n",
      "379 1766.3666362845916\n",
      "380 1755.2780503619274\n",
      "381 1744.2426337771994\n",
      "382 1733.405284281879\n",
      "383 1722.7176328560831\n",
      "384 1712.1256324658973\n",
      "385 1701.6233024221578\n",
      "386 1691.1995653822348\n",
      "387 1680.880981112191\n",
      "388 1670.6686237515119\n",
      "389 1660.5309748184309\n",
      "390 1650.4759638929004\n",
      "391 1640.498624896601\n",
      "392 1630.5941912510157\n",
      "393 1620.7973065064427\n",
      "394 1611.056079035802\n",
      "395 1601.4439748167977\n",
      "396 1591.8760636227737\n",
      "397 1582.3996262451003\n",
      "398 1572.9821651732998\n",
      "399 1563.6358390189282\n",
      "400 1554.3734075155899\n",
      "401 1545.1933787207913\n",
      "402 1536.0813691111523\n",
      "403 1527.0396764603888\n",
      "404 1518.1199154760775\n",
      "405 1509.236118638707\n",
      "406 1500.450822520913\n",
      "407 1491.721084638504\n",
      "408 1483.0463170034684\n",
      "409 1474.4347459617863\n",
      "410 1465.9073358304192\n",
      "411 1457.4446363591344\n",
      "412 1449.0540726564996\n",
      "413 1440.7282893207484\n",
      "414 1432.4534471740176\n",
      "415 1424.248545926202\n",
      "416 1416.1267519653497\n",
      "417 1408.0738347135512\n",
      "418 1400.059568250381\n",
      "419 1392.1249771391206\n",
      "420 1384.2632992564434\n",
      "421 1376.4483048671664\n",
      "422 1368.6949202473324\n",
      "423 1361.0620594834766\n",
      "424 1353.5449273941413\n",
      "425 1346.0848267387514\n",
      "426 1338.685674908285\n",
      "427 1331.3330466843354\n",
      "428 1324.0404153711663\n",
      "429 1316.8160310475696\n",
      "430 1309.6500800958545\n",
      "431 1302.529202347679\n",
      "432 1295.473195100609\n",
      "433 1288.4637541533875\n",
      "434 1281.4942143328017\n",
      "435 1274.578166193058\n",
      "436 1267.7115183843239\n",
      "437 1260.9041369417866\n",
      "438 1254.1623170887713\n",
      "439 1247.4630446906492\n",
      "440 1240.8054830376298\n",
      "441 1234.1962346090256\n",
      "442 1227.6356918247827\n",
      "443 1221.1273565422296\n",
      "444 1214.6655954037276\n",
      "445 1208.2451637873173\n",
      "446 1201.8913164423207\n",
      "447 1195.5866588266847\n",
      "448 1189.3210002974179\n",
      "449 1183.091408344659\n",
      "450 1176.9045348006903\n",
      "451 1170.7643265116446\n",
      "452 1164.6708278253818\n",
      "453 1158.6255816696791\n",
      "454 1152.6184823917315\n",
      "455 1146.6552873880578\n",
      "456 1140.7332301715494\n",
      "457 1134.8634676442248\n",
      "458 1129.02275097536\n",
      "459 1123.2406381196176\n",
      "460 1117.5138716034933\n",
      "461 1111.7947025816513\n",
      "462 1106.1136860519173\n",
      "463 1100.476513985241\n",
      "464 1094.890753711431\n",
      "465 1089.3385403290836\n",
      "466 1083.8368753105026\n",
      "467 1078.357694276642\n",
      "468 1072.9148727911975\n",
      "469 1067.5105833526195\n",
      "470 1062.1484417942206\n",
      "471 1056.817975132847\n",
      "472 1051.5264878852497\n",
      "473 1046.2716486420707\n",
      "474 1041.0735468229554\n",
      "475 1035.9026762494364\n",
      "476 1030.7799725239577\n",
      "477 1025.676574773979\n",
      "478 1020.598777371742\n",
      "479 1015.5584350795776\n",
      "480 1010.5774180145623\n",
      "481 1005.6227301829035\n",
      "482 1000.7028745857291\n",
      "483 995.8194909887051\n",
      "484 990.9717861432209\n",
      "485 986.1572916203505\n",
      "486 981.3968643905733\n",
      "487 976.6508406291088\n",
      "488 972.0514030845528\n",
      "489 967.4826728315004\n",
      "490 962.9385260530009\n",
      "491 958.4157300677919\n",
      "492 953.9258287584646\n",
      "493 949.4648220228604\n",
      "494 945.0387730139366\n",
      "495 940.6548089822094\n",
      "496 936.2952117381228\n",
      "497 931.9462487234514\n",
      "498 927.6291995935055\n",
      "499 923.3383710805053\n"
     ]
    }
   ],
   "source": [
    "# Numpy Version\n",
    "import numpy as np\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 1\n",
    "\n",
    "# generate the training data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# init the weights\n",
    "w1 = np.random.randn(D_in, D_h)\n",
    "w2 = np.random.randn(D_h, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    # calculate the loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    # back-propagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    # update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 39354416.0\n",
      "1 69728016.0\n",
      "2 301565664.0\n",
      "3 923355520.0\n",
      "4 183074496.0\n",
      "5 25839176.0\n",
      "6 15219064.0\n",
      "7 10511831.0\n",
      "8 7848824.5\n",
      "9 6157910.0\n",
      "10 4996657.5\n",
      "11 4156885.0\n",
      "12 3524331.25\n",
      "13 3033465.75\n",
      "14 2642229.0\n",
      "15 2325109.5\n",
      "16 2063716.25\n",
      "17 1845775.125\n",
      "18 1661550.0\n",
      "19 1503752.5\n",
      "20 1367599.0\n",
      "21 1249529.0\n",
      "22 1146554.625\n",
      "23 1056163.375\n",
      "24 976136.3125\n",
      "25 904936.25\n",
      "26 841283.3125\n",
      "27 784326.25\n",
      "28 733280.8125\n",
      "29 687390.0\n",
      "30 645892.625\n",
      "31 608086.9375\n",
      "32 573518.625\n",
      "33 541881.875\n",
      "34 512836.5625\n",
      "35 486059.75\n",
      "36 461372.15625\n",
      "37 438537.875\n",
      "38 417399.1875\n",
      "39 397815.375\n",
      "40 379584.1875\n",
      "41 362584.40625\n",
      "42 346714.1875\n",
      "43 331869.0\n",
      "44 317965.6875\n",
      "45 304922.25\n",
      "46 292671.0\n",
      "47 281178.21875\n",
      "48 270373.125\n",
      "49 260199.8125\n",
      "50 250595.40625\n",
      "51 241511.3125\n",
      "52 232917.46875\n",
      "53 224778.8125\n",
      "54 217060.65625\n",
      "55 209739.65625\n",
      "56 202787.640625\n",
      "57 196178.828125\n",
      "58 189892.375\n",
      "59 183890.234375\n",
      "60 178138.84375\n",
      "61 172657.3125\n",
      "62 167426.96875\n",
      "63 162432.53125\n",
      "64 157664.234375\n",
      "65 153113.453125\n",
      "66 148768.953125\n",
      "67 144601.5\n",
      "68 140622.46875\n",
      "69 136814.125\n",
      "70 133168.828125\n",
      "71 129662.8203125\n",
      "72 126295.046875\n",
      "73 123059.78125\n",
      "74 119950.546875\n",
      "75 116962.453125\n",
      "76 114088.75\n",
      "77 111328.796875\n",
      "78 108673.8203125\n",
      "79 106117.03125\n",
      "80 103653.65625\n",
      "81 101270.1640625\n",
      "82 98970.65625\n",
      "83 96749.8515625\n",
      "84 94606.875\n",
      "85 92538.046875\n",
      "86 90538.21875\n",
      "87 88604.78125\n",
      "88 86733.171875\n",
      "89 84922.625\n",
      "90 83170.890625\n",
      "91 81474.671875\n",
      "92 79832.484375\n",
      "93 78242.671875\n",
      "94 76700.609375\n",
      "95 75205.65625\n",
      "96 73757.03125\n",
      "97 72351.2578125\n",
      "98 70986.953125\n",
      "99 69662.75\n",
      "100 68376.6796875\n",
      "101 67127.3671875\n",
      "102 65910.4296875\n",
      "103 64728.484375\n",
      "104 63579.5703125\n",
      "105 62462.109375\n",
      "106 61378.67578125\n",
      "107 60323.9921875\n",
      "108 59297.7890625\n",
      "109 58299.87109375\n",
      "110 57327.68359375\n",
      "111 56380.91796875\n",
      "112 55458.16015625\n",
      "113 54559.9453125\n",
      "114 53686.84375\n",
      "115 52834.25390625\n",
      "116 52003.42578125\n",
      "117 51194.4765625\n",
      "118 50406.5234375\n",
      "119 49637.640625\n",
      "120 48887.7421875\n",
      "121 48155.9609375\n",
      "122 47441.5390625\n",
      "123 46743.921875\n",
      "124 46060.4296875\n",
      "125 45392.75\n",
      "126 44740.03125\n",
      "127 44102.40234375\n",
      "128 43479.203125\n",
      "129 42870.390625\n",
      "130 42274.69140625\n",
      "131 41692.4453125\n",
      "132 41123.40234375\n",
      "133 40567.68359375\n",
      "134 40023.7265625\n",
      "135 39491.94921875\n",
      "136 38971.328125\n",
      "137 38462.34375\n",
      "138 37963.546875\n",
      "139 37474.99609375\n",
      "140 36997.7578125\n",
      "141 36530.75\n",
      "142 36073.09375\n",
      "143 35625.1875\n",
      "144 35186.2734375\n",
      "145 34756.2578125\n",
      "146 34334.703125\n",
      "147 33921.71875\n",
      "148 33516.65625\n",
      "149 33119.46484375\n",
      "150 32730.1796875\n",
      "151 32348.3984375\n",
      "152 31973.6015625\n",
      "153 31606.05078125\n",
      "154 31245.171875\n",
      "155 30890.970703125\n",
      "156 30543.259765625\n",
      "157 30201.826171875\n",
      "158 29866.33203125\n",
      "159 29536.892578125\n",
      "160 29213.30859375\n",
      "161 28895.98046875\n",
      "162 28584.19921875\n",
      "163 28278.08203125\n",
      "164 27977.287109375\n",
      "165 27681.4921875\n",
      "166 27390.841796875\n",
      "167 27105.236328125\n",
      "168 26824.314453125\n",
      "169 26547.669921875\n",
      "170 26275.900390625\n",
      "171 26009.037109375\n",
      "172 25746.32421875\n",
      "173 25488.14453125\n",
      "174 25234.205078125\n",
      "175 24984.50390625\n",
      "176 24738.90625\n",
      "177 24497.439453125\n",
      "178 24259.517578125\n",
      "179 24025.44140625\n",
      "180 23795.08984375\n",
      "181 23568.2890625\n",
      "182 23345.23828125\n",
      "183 23125.703125\n",
      "184 22909.48046875\n",
      "185 22696.470703125\n",
      "186 22486.595703125\n",
      "187 22279.96875\n",
      "188 22076.5234375\n",
      "189 21875.990234375\n",
      "190 21678.56640625\n",
      "191 21481.0\n",
      "192 21286.5546875\n",
      "193 21095.0703125\n",
      "194 20906.33203125\n",
      "195 20720.283203125\n",
      "196 20537.037109375\n",
      "197 20356.40625\n",
      "198 20178.3828125\n",
      "199 20002.9140625\n",
      "200 19829.943359375\n",
      "201 19659.37109375\n",
      "202 19491.267578125\n",
      "203 19325.419921875\n",
      "204 19162.015625\n",
      "205 19000.9375\n",
      "206 18842.255859375\n",
      "207 18685.734375\n",
      "208 18531.3984375\n",
      "209 18379.240234375\n",
      "210 18229.041015625\n",
      "211 18080.9765625\n",
      "212 17934.810546875\n",
      "213 17790.580078125\n",
      "214 17648.25390625\n",
      "215 17507.716796875\n",
      "216 17369.0390625\n",
      "217 17232.107421875\n",
      "218 17097.1171875\n",
      "219 16963.76953125\n",
      "220 16832.1484375\n",
      "221 16702.224609375\n",
      "222 16573.984375\n",
      "223 16447.296875\n",
      "224 16322.224609375\n",
      "225 16198.4345703125\n",
      "226 16076.140625\n",
      "227 15955.318359375\n",
      "228 15835.974609375\n",
      "229 15717.998046875\n",
      "230 15601.453125\n",
      "231 15484.083984375\n",
      "232 15368.2646484375\n",
      "233 15253.8505859375\n",
      "234 15140.880859375\n",
      "235 15029.416015625\n",
      "236 14919.3525390625\n",
      "237 14810.6484375\n",
      "238 14703.333984375\n",
      "239 14597.2412109375\n",
      "240 14492.56640625\n",
      "241 14389.1796875\n",
      "242 14287.13671875\n",
      "243 14186.2138671875\n",
      "244 14086.396484375\n",
      "245 13987.79296875\n",
      "246 13890.3603515625\n",
      "247 13794.048828125\n",
      "248 13698.880859375\n",
      "249 13604.8251953125\n",
      "250 13511.8330078125\n",
      "251 13419.9208984375\n",
      "252 13329.005859375\n",
      "253 13239.083984375\n",
      "254 13150.197265625\n",
      "255 13062.2900390625\n",
      "256 12975.30859375\n",
      "257 12889.333984375\n",
      "258 12804.2822265625\n",
      "259 12720.150390625\n",
      "260 12636.939453125\n",
      "261 12554.6708984375\n",
      "262 12473.2177734375\n",
      "263 12392.640625\n",
      "264 12312.8818359375\n",
      "265 12234.01953125\n",
      "266 12155.9970703125\n",
      "267 12078.7568359375\n",
      "268 12002.3505859375\n",
      "269 11926.74609375\n",
      "270 11851.9052734375\n",
      "271 11777.84375\n",
      "272 11704.587890625\n",
      "273 11632.052734375\n",
      "274 11560.3203125\n",
      "275 11489.25390625\n",
      "276 11418.853515625\n",
      "277 11349.16796875\n",
      "278 11280.23828125\n",
      "279 11211.90234375\n",
      "280 11144.2734375\n",
      "281 11077.296875\n",
      "282 11010.9853515625\n",
      "283 10945.35546875\n",
      "284 10880.322265625\n",
      "285 10815.9052734375\n",
      "286 10751.9794921875\n",
      "287 10688.6591796875\n",
      "288 10625.9873046875\n",
      "289 10563.921875\n",
      "290 10502.4091796875\n",
      "291 10441.5322265625\n",
      "292 10381.2509765625\n",
      "293 10321.46484375\n",
      "294 10262.236328125\n",
      "295 10203.5673828125\n",
      "296 10145.390625\n",
      "297 10087.783203125\n",
      "298 10030.712890625\n",
      "299 9974.203125\n",
      "300 9918.1875\n",
      "301 9862.671875\n",
      "302 9807.646484375\n",
      "303 9753.12109375\n",
      "304 9699.083984375\n",
      "305 9645.5361328125\n",
      "306 9592.4833984375\n",
      "307 9539.921875\n",
      "308 9487.865234375\n",
      "309 9436.201171875\n",
      "310 9385.0390625\n",
      "311 9334.3544921875\n",
      "312 9284.15234375\n",
      "313 9234.416015625\n",
      "314 9185.013671875\n",
      "315 9135.970703125\n",
      "316 9087.345703125\n",
      "317 9039.1298828125\n",
      "318 8991.34765625\n",
      "319 8943.9609375\n",
      "320 8896.990234375\n",
      "321 8850.423828125\n",
      "322 8804.251953125\n",
      "323 8758.453125\n",
      "324 8713.03125\n",
      "325 8668.0078125\n",
      "326 8623.33984375\n",
      "327 8579.04296875\n",
      "328 8535.1064453125\n",
      "329 8491.572265625\n",
      "330 8448.3857421875\n",
      "331 8405.572265625\n",
      "332 8363.0673828125\n",
      "333 8320.9306640625\n",
      "334 8279.1484375\n",
      "335 8237.6767578125\n",
      "336 8196.5302734375\n",
      "337 8155.71484375\n",
      "338 8115.22119140625\n",
      "339 8075.05419921875\n",
      "340 8035.06201171875\n",
      "341 7995.42529296875\n",
      "342 7956.09423828125\n",
      "343 7917.04296875\n",
      "344 7878.3115234375\n",
      "345 7839.90283203125\n",
      "346 7801.794921875\n",
      "347 7763.970703125\n",
      "348 7726.46826171875\n",
      "349 7689.2275390625\n",
      "350 7652.2783203125\n",
      "351 7615.61083984375\n",
      "352 7579.259765625\n",
      "353 7543.1884765625\n",
      "354 7507.39453125\n",
      "355 7471.9033203125\n",
      "356 7436.685546875\n",
      "357 7401.728515625\n",
      "358 7367.0458984375\n",
      "359 7332.62646484375\n",
      "360 7298.4765625\n",
      "361 7264.6015625\n",
      "362 7230.96484375\n",
      "363 7197.583984375\n",
      "364 7164.4658203125\n",
      "365 7131.57373046875\n",
      "366 7098.9306640625\n",
      "367 7066.5517578125\n",
      "368 7034.3935546875\n",
      "369 7002.5029296875\n",
      "370 6970.85400390625\n",
      "371 6939.3935546875\n",
      "372 6908.1767578125\n",
      "373 6877.18359375\n",
      "374 6846.4228515625\n",
      "375 6815.875\n",
      "376 6785.5458984375\n",
      "377 6755.4345703125\n",
      "378 6725.537109375\n",
      "379 6695.841796875\n",
      "380 6666.3740234375\n",
      "381 6637.138671875\n",
      "382 6608.0810546875\n",
      "383 6579.2412109375\n",
      "384 6550.60888671875\n",
      "385 6522.18603515625\n",
      "386 6493.94384765625\n",
      "387 6465.89697265625\n",
      "388 6438.021484375\n",
      "389 6410.37353515625\n",
      "390 6382.88916015625\n",
      "391 6355.5908203125\n",
      "392 6328.47021484375\n",
      "393 6301.5458984375\n",
      "394 6274.80859375\n",
      "395 6248.23779296875\n",
      "396 6221.8564453125\n",
      "397 6195.6513671875\n",
      "398 6169.60498046875\n",
      "399 6143.74658203125\n",
      "400 6118.05615234375\n",
      "401 6092.55615234375\n",
      "402 6067.2373046875\n",
      "403 6042.09716796875\n",
      "404 6017.09326171875\n",
      "405 5992.265625\n",
      "406 5967.5869140625\n",
      "407 5943.0625\n",
      "408 5918.70556640625\n",
      "409 5894.482421875\n",
      "410 5870.44921875\n",
      "411 5846.56640625\n",
      "412 5822.84375\n",
      "413 5799.25634765625\n",
      "414 5775.82861328125\n",
      "415 5752.55517578125\n",
      "416 5729.44775390625\n",
      "417 5706.462890625\n",
      "418 5683.63232421875\n",
      "419 5660.94287109375\n",
      "420 5638.3876953125\n",
      "421 5615.9697265625\n",
      "422 5593.6953125\n",
      "423 5571.5625\n",
      "424 5549.5947265625\n",
      "425 5527.78076171875\n",
      "426 5506.080078125\n",
      "427 5484.5185546875\n",
      "428 5463.08447265625\n",
      "429 5441.78564453125\n",
      "430 5420.65087890625\n",
      "431 5399.609375\n",
      "432 5378.70068359375\n",
      "433 5357.919921875\n",
      "434 5337.25927734375\n",
      "435 5316.7431640625\n",
      "436 5296.34375\n",
      "437 5276.0615234375\n",
      "438 5255.9033203125\n",
      "439 5235.8671875\n",
      "440 5215.9580078125\n",
      "441 5196.158203125\n",
      "442 5176.138671875\n",
      "443 5156.2451171875\n",
      "444 5136.4951171875\n",
      "445 5116.888671875\n",
      "446 5097.3779296875\n",
      "447 5077.9921875\n",
      "448 5058.716796875\n",
      "449 5039.56494140625\n",
      "450 5020.52734375\n",
      "451 5001.6220703125\n",
      "452 4982.82861328125\n",
      "453 4964.158203125\n",
      "454 4945.58056640625\n",
      "455 4927.1279296875\n",
      "456 4908.7744140625\n",
      "457 4890.2998046875\n",
      "458 4871.939453125\n",
      "459 4853.7099609375\n",
      "460 4835.5859375\n",
      "461 4817.58203125\n",
      "462 4799.6767578125\n",
      "463 4781.876953125\n",
      "464 4764.18359375\n",
      "465 4746.6123046875\n",
      "466 4729.1494140625\n",
      "467 4711.7744140625\n",
      "468 4694.49560546875\n",
      "469 4677.3251953125\n",
      "470 4660.2578125\n",
      "471 4643.30419921875\n",
      "472 4626.443359375\n",
      "473 4609.673828125\n",
      "474 4593.0048828125\n",
      "475 4576.4306640625\n",
      "476 4559.94921875\n",
      "477 4543.57373046875\n",
      "478 4527.294921875\n",
      "479 4511.10888671875\n",
      "480 4495.009765625\n",
      "481 4479.00244140625\n",
      "482 4463.083984375\n",
      "483 4447.2509765625\n",
      "484 4431.5078125\n",
      "485 4415.8603515625\n",
      "486 4400.29638671875\n",
      "487 4384.8212890625\n",
      "488 4369.44189453125\n",
      "489 4354.13671875\n",
      "490 4338.91796875\n",
      "491 4323.75390625\n",
      "492 4308.66943359375\n",
      "493 4293.66845703125\n",
      "494 4278.75537109375\n",
      "495 4263.921875\n",
      "496 4249.16259765625\n",
      "497 4234.4921875\n",
      "498 4219.89208984375\n",
      "499 4205.373046875\n"
     ]
    }
   ],
   "source": [
    "# Tensor Version\n",
    "import torch\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate the training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# init the weights\n",
    "w1 = torch.randn(D_in, D_h, device=device)\n",
    "w2 = torch.randn(D_h, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    # calculate the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    # back-propagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    # update the weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(38188340., grad_fn=<SumBackward0>)\n",
      "1 tensor(49341676., grad_fn=<SumBackward0>)\n",
      "2 tensor(1.6171e+08, grad_fn=<SumBackward0>)\n",
      "3 tensor(5.3576e+08, grad_fn=<SumBackward0>)\n",
      "4 tensor(4.8901e+08, grad_fn=<SumBackward0>)\n",
      "5 tensor(2362256.5000, grad_fn=<SumBackward0>)\n",
      "6 tensor(2021672.2500, grad_fn=<SumBackward0>)\n",
      "7 tensor(1762249.7500, grad_fn=<SumBackward0>)\n",
      "8 tensor(1557750., grad_fn=<SumBackward0>)\n",
      "9 tensor(1390408., grad_fn=<SumBackward0>)\n",
      "10 tensor(1250695.7500, grad_fn=<SumBackward0>)\n",
      "11 tensor(1132055.5000, grad_fn=<SumBackward0>)\n",
      "12 tensor(1029826.6250, grad_fn=<SumBackward0>)\n",
      "13 tensor(940923.1875, grad_fn=<SumBackward0>)\n",
      "14 tensor(862830.5000, grad_fn=<SumBackward0>)\n",
      "15 tensor(793863.1250, grad_fn=<SumBackward0>)\n",
      "16 tensor(732542.1875, grad_fn=<SumBackward0>)\n",
      "17 tensor(677760., grad_fn=<SumBackward0>)\n",
      "18 tensor(628659.5625, grad_fn=<SumBackward0>)\n",
      "19 tensor(584430., grad_fn=<SumBackward0>)\n",
      "20 tensor(544439.7500, grad_fn=<SumBackward0>)\n",
      "21 tensor(508163.7812, grad_fn=<SumBackward0>)\n",
      "22 tensor(475272.1250, grad_fn=<SumBackward0>)\n",
      "23 tensor(445292., grad_fn=<SumBackward0>)\n",
      "24 tensor(417912.1875, grad_fn=<SumBackward0>)\n",
      "25 tensor(392883.4688, grad_fn=<SumBackward0>)\n",
      "26 tensor(369909.9062, grad_fn=<SumBackward0>)\n",
      "27 tensor(348778.6250, grad_fn=<SumBackward0>)\n",
      "28 tensor(329297.3125, grad_fn=<SumBackward0>)\n",
      "29 tensor(311344.0938, grad_fn=<SumBackward0>)\n",
      "30 tensor(294733.7812, grad_fn=<SumBackward0>)\n",
      "31 tensor(279364.6562, grad_fn=<SumBackward0>)\n",
      "32 tensor(265104.5938, grad_fn=<SumBackward0>)\n",
      "33 tensor(251897.1719, grad_fn=<SumBackward0>)\n",
      "34 tensor(239548.8125, grad_fn=<SumBackward0>)\n",
      "35 tensor(228027.1250, grad_fn=<SumBackward0>)\n",
      "36 tensor(217281.5625, grad_fn=<SumBackward0>)\n",
      "37 tensor(207245.0781, grad_fn=<SumBackward0>)\n",
      "38 tensor(197852.6875, grad_fn=<SumBackward0>)\n",
      "39 tensor(189046.8125, grad_fn=<SumBackward0>)\n",
      "40 tensor(180773.5000, grad_fn=<SumBackward0>)\n",
      "41 tensor(172990.5625, grad_fn=<SumBackward0>)\n",
      "42 tensor(165671.5625, grad_fn=<SumBackward0>)\n",
      "43 tensor(158778.7969, grad_fn=<SumBackward0>)\n",
      "44 tensor(152282.5156, grad_fn=<SumBackward0>)\n",
      "45 tensor(146153.9688, grad_fn=<SumBackward0>)\n",
      "46 tensor(140364.6406, grad_fn=<SumBackward0>)\n",
      "47 tensor(134890.9844, grad_fn=<SumBackward0>)\n",
      "48 tensor(129716.3125, grad_fn=<SumBackward0>)\n",
      "49 tensor(124818.5391, grad_fn=<SumBackward0>)\n",
      "50 tensor(120175.8594, grad_fn=<SumBackward0>)\n",
      "51 tensor(115770.4922, grad_fn=<SumBackward0>)\n",
      "52 tensor(111588.2891, grad_fn=<SumBackward0>)\n",
      "53 tensor(107614.4062, grad_fn=<SumBackward0>)\n",
      "54 tensor(103837.4609, grad_fn=<SumBackward0>)\n",
      "55 tensor(100241.9688, grad_fn=<SumBackward0>)\n",
      "56 tensor(96818.9375, grad_fn=<SumBackward0>)\n",
      "57 tensor(93556.8906, grad_fn=<SumBackward0>)\n",
      "58 tensor(90446.6094, grad_fn=<SumBackward0>)\n",
      "59 tensor(87477.2344, grad_fn=<SumBackward0>)\n",
      "60 tensor(84640.6641, grad_fn=<SumBackward0>)\n",
      "61 tensor(81931.7969, grad_fn=<SumBackward0>)\n",
      "62 tensor(79340.8047, grad_fn=<SumBackward0>)\n",
      "63 tensor(76865.0234, grad_fn=<SumBackward0>)\n",
      "64 tensor(74492.7578, grad_fn=<SumBackward0>)\n",
      "65 tensor(72220.7266, grad_fn=<SumBackward0>)\n",
      "66 tensor(70043.8594, grad_fn=<SumBackward0>)\n",
      "67 tensor(67956.8125, grad_fn=<SumBackward0>)\n",
      "68 tensor(65954.2891, grad_fn=<SumBackward0>)\n",
      "69 tensor(64031.8750, grad_fn=<SumBackward0>)\n",
      "70 tensor(62185.8906, grad_fn=<SumBackward0>)\n",
      "71 tensor(60411.9492, grad_fn=<SumBackward0>)\n",
      "72 tensor(58706.7383, grad_fn=<SumBackward0>)\n",
      "73 tensor(57066.8359, grad_fn=<SumBackward0>)\n",
      "74 tensor(55488.9727, grad_fn=<SumBackward0>)\n",
      "75 tensor(53970.7812, grad_fn=<SumBackward0>)\n",
      "76 tensor(52508.4531, grad_fn=<SumBackward0>)\n",
      "77 tensor(51100.4766, grad_fn=<SumBackward0>)\n",
      "78 tensor(49745.1641, grad_fn=<SumBackward0>)\n",
      "79 tensor(48439.7812, grad_fn=<SumBackward0>)\n",
      "80 tensor(47180.5352, grad_fn=<SumBackward0>)\n",
      "81 tensor(45965.0938, grad_fn=<SumBackward0>)\n",
      "82 tensor(44791.9766, grad_fn=<SumBackward0>)\n",
      "83 tensor(43659.5938, grad_fn=<SumBackward0>)\n",
      "84 tensor(42568.5156, grad_fn=<SumBackward0>)\n",
      "85 tensor(41515.8398, grad_fn=<SumBackward0>)\n",
      "86 tensor(40495.8008, grad_fn=<SumBackward0>)\n",
      "87 tensor(39510.2891, grad_fn=<SumBackward0>)\n",
      "88 tensor(38557.3633, grad_fn=<SumBackward0>)\n",
      "89 tensor(37635.2969, grad_fn=<SumBackward0>)\n",
      "90 tensor(36743.4219, grad_fn=<SumBackward0>)\n",
      "91 tensor(35880.2891, grad_fn=<SumBackward0>)\n",
      "92 tensor(35044.6953, grad_fn=<SumBackward0>)\n",
      "93 tensor(34235.8945, grad_fn=<SumBackward0>)\n",
      "94 tensor(33452.4375, grad_fn=<SumBackward0>)\n",
      "95 tensor(32692.9902, grad_fn=<SumBackward0>)\n",
      "96 tensor(31956.8594, grad_fn=<SumBackward0>)\n",
      "97 tensor(31242.9258, grad_fn=<SumBackward0>)\n",
      "98 tensor(30551.2383, grad_fn=<SumBackward0>)\n",
      "99 tensor(29881.1953, grad_fn=<SumBackward0>)\n",
      "100 tensor(29231.2852, grad_fn=<SumBackward0>)\n",
      "101 tensor(28600.2812, grad_fn=<SumBackward0>)\n",
      "102 tensor(27987.7578, grad_fn=<SumBackward0>)\n",
      "103 tensor(27392.6562, grad_fn=<SumBackward0>)\n",
      "104 tensor(26814.6719, grad_fn=<SumBackward0>)\n",
      "105 tensor(26253.1211, grad_fn=<SumBackward0>)\n",
      "106 tensor(25707.2188, grad_fn=<SumBackward0>)\n",
      "107 tensor(25176.4297, grad_fn=<SumBackward0>)\n",
      "108 tensor(24660.5391, grad_fn=<SumBackward0>)\n",
      "109 tensor(24158.7402, grad_fn=<SumBackward0>)\n",
      "110 tensor(23670.8105, grad_fn=<SumBackward0>)\n",
      "111 tensor(23195.5566, grad_fn=<SumBackward0>)\n",
      "112 tensor(22733.4473, grad_fn=<SumBackward0>)\n",
      "113 tensor(22283.9824, grad_fn=<SumBackward0>)\n",
      "114 tensor(21846.1250, grad_fn=<SumBackward0>)\n",
      "115 tensor(21419.8574, grad_fn=<SumBackward0>)\n",
      "116 tensor(21005.0273, grad_fn=<SumBackward0>)\n",
      "117 tensor(20600.9297, grad_fn=<SumBackward0>)\n",
      "118 tensor(20207.3633, grad_fn=<SumBackward0>)\n",
      "119 tensor(19824.3164, grad_fn=<SumBackward0>)\n",
      "120 tensor(19451.1484, grad_fn=<SumBackward0>)\n",
      "121 tensor(19087.1758, grad_fn=<SumBackward0>)\n",
      "122 tensor(18732.4121, grad_fn=<SumBackward0>)\n",
      "123 tensor(18386.8848, grad_fn=<SumBackward0>)\n",
      "124 tensor(18050.3242, grad_fn=<SumBackward0>)\n",
      "125 tensor(17721.7832, grad_fn=<SumBackward0>)\n",
      "126 tensor(17401.1641, grad_fn=<SumBackward0>)\n",
      "127 tensor(17088.2559, grad_fn=<SumBackward0>)\n",
      "128 tensor(16782.8926, grad_fn=<SumBackward0>)\n",
      "129 tensor(16484.6738, grad_fn=<SumBackward0>)\n",
      "130 tensor(16193.5332, grad_fn=<SumBackward0>)\n",
      "131 tensor(15909.2139, grad_fn=<SumBackward0>)\n",
      "132 tensor(15631.4805, grad_fn=<SumBackward0>)\n",
      "133 tensor(15360.0986, grad_fn=<SumBackward0>)\n",
      "134 tensor(15094.9688, grad_fn=<SumBackward0>)\n",
      "135 tensor(14835.8848, grad_fn=<SumBackward0>)\n",
      "136 tensor(14582.7256, grad_fn=<SumBackward0>)\n",
      "137 tensor(14335.2021, grad_fn=<SumBackward0>)\n",
      "138 tensor(14093.0605, grad_fn=<SumBackward0>)\n",
      "139 tensor(13856.4531, grad_fn=<SumBackward0>)\n",
      "140 tensor(13625.0410, grad_fn=<SumBackward0>)\n",
      "141 tensor(13398.6602, grad_fn=<SumBackward0>)\n",
      "142 tensor(13177.2197, grad_fn=<SumBackward0>)\n",
      "143 tensor(12960.5293, grad_fn=<SumBackward0>)\n",
      "144 tensor(12748.5918, grad_fn=<SumBackward0>)\n",
      "145 tensor(12541.1201, grad_fn=<SumBackward0>)\n",
      "146 tensor(12338.0771, grad_fn=<SumBackward0>)\n",
      "147 tensor(12139.5615, grad_fn=<SumBackward0>)\n",
      "148 tensor(11945.2129, grad_fn=<SumBackward0>)\n",
      "149 tensor(11754.8730, grad_fn=<SumBackward0>)\n",
      "150 tensor(11568.4922, grad_fn=<SumBackward0>)\n",
      "151 tensor(11386.0039, grad_fn=<SumBackward0>)\n",
      "152 tensor(11207.2949, grad_fn=<SumBackward0>)\n",
      "153 tensor(11032.1445, grad_fn=<SumBackward0>)\n",
      "154 tensor(10860.4463, grad_fn=<SumBackward0>)\n",
      "155 tensor(10692.2627, grad_fn=<SumBackward0>)\n",
      "156 tensor(10527.4922, grad_fn=<SumBackward0>)\n",
      "157 tensor(10365.9814, grad_fn=<SumBackward0>)\n",
      "158 tensor(10207.6885, grad_fn=<SumBackward0>)\n",
      "159 tensor(10052.6445, grad_fn=<SumBackward0>)\n",
      "160 tensor(9900.7471, grad_fn=<SumBackward0>)\n",
      "161 tensor(9751.7920, grad_fn=<SumBackward0>)\n",
      "162 tensor(9605.7480, grad_fn=<SumBackward0>)\n",
      "163 tensor(9462.5000, grad_fn=<SumBackward0>)\n",
      "164 tensor(9321.9580, grad_fn=<SumBackward0>)\n",
      "165 tensor(9184.0723, grad_fn=<SumBackward0>)\n",
      "166 tensor(9048.8115, grad_fn=<SumBackward0>)\n",
      "167 tensor(8916.2285, grad_fn=<SumBackward0>)\n",
      "168 tensor(8786.3877, grad_fn=<SumBackward0>)\n",
      "169 tensor(8659.1162, grad_fn=<SumBackward0>)\n",
      "170 tensor(8534.2305, grad_fn=<SumBackward0>)\n",
      "171 tensor(8411.6660, grad_fn=<SumBackward0>)\n",
      "172 tensor(8291.4180, grad_fn=<SumBackward0>)\n",
      "173 tensor(8173.3896, grad_fn=<SumBackward0>)\n",
      "174 tensor(8057.5098, grad_fn=<SumBackward0>)\n",
      "175 tensor(7943.6953, grad_fn=<SumBackward0>)\n",
      "176 tensor(7832.0225, grad_fn=<SumBackward0>)\n",
      "177 tensor(7722.2915, grad_fn=<SumBackward0>)\n",
      "178 tensor(7614.5386, grad_fn=<SumBackward0>)\n",
      "179 tensor(7508.7134, grad_fn=<SumBackward0>)\n",
      "180 tensor(7404.7285, grad_fn=<SumBackward0>)\n",
      "181 tensor(7302.5967, grad_fn=<SumBackward0>)\n",
      "182 tensor(7202.2461, grad_fn=<SumBackward0>)\n",
      "183 tensor(7103.6411, grad_fn=<SumBackward0>)\n",
      "184 tensor(7006.8345, grad_fn=<SumBackward0>)\n",
      "185 tensor(6911.6411, grad_fn=<SumBackward0>)\n",
      "186 tensor(6818.0723, grad_fn=<SumBackward0>)\n",
      "187 tensor(6726.2534, grad_fn=<SumBackward0>)\n",
      "188 tensor(6636.0781, grad_fn=<SumBackward0>)\n",
      "189 tensor(6547.4399, grad_fn=<SumBackward0>)\n",
      "190 tensor(6460.3018, grad_fn=<SumBackward0>)\n",
      "191 tensor(6374.6157, grad_fn=<SumBackward0>)\n",
      "192 tensor(6290.3804, grad_fn=<SumBackward0>)\n",
      "193 tensor(6207.5527, grad_fn=<SumBackward0>)\n",
      "194 tensor(6126.1172, grad_fn=<SumBackward0>)\n",
      "195 tensor(6046.0312, grad_fn=<SumBackward0>)\n",
      "196 tensor(5967.2529, grad_fn=<SumBackward0>)\n",
      "197 tensor(5889.8740, grad_fn=<SumBackward0>)\n",
      "198 tensor(5813.6812, grad_fn=<SumBackward0>)\n",
      "199 tensor(5738.7031, grad_fn=<SumBackward0>)\n",
      "200 tensor(5664.9688, grad_fn=<SumBackward0>)\n",
      "201 tensor(5592.4360, grad_fn=<SumBackward0>)\n",
      "202 tensor(5521.0449, grad_fn=<SumBackward0>)\n",
      "203 tensor(5450.8262, grad_fn=<SumBackward0>)\n",
      "204 tensor(5381.7290, grad_fn=<SumBackward0>)\n",
      "205 tensor(5313.7588, grad_fn=<SumBackward0>)\n",
      "206 tensor(5246.8335, grad_fn=<SumBackward0>)\n",
      "207 tensor(5180.9824, grad_fn=<SumBackward0>)\n",
      "208 tensor(5116.1709, grad_fn=<SumBackward0>)\n",
      "209 tensor(5052.3730, grad_fn=<SumBackward0>)\n",
      "210 tensor(4989.6284, grad_fn=<SumBackward0>)\n",
      "211 tensor(4927.8047, grad_fn=<SumBackward0>)\n",
      "212 tensor(4866.9385, grad_fn=<SumBackward0>)\n",
      "213 tensor(4807.0264, grad_fn=<SumBackward0>)\n",
      "214 tensor(4748.0352, grad_fn=<SumBackward0>)\n",
      "215 tensor(4689.9336, grad_fn=<SumBackward0>)\n",
      "216 tensor(4632.7314, grad_fn=<SumBackward0>)\n",
      "217 tensor(4576.4199, grad_fn=<SumBackward0>)\n",
      "218 tensor(4520.9570, grad_fn=<SumBackward0>)\n",
      "219 tensor(4466.3154, grad_fn=<SumBackward0>)\n",
      "220 tensor(4412.4990, grad_fn=<SumBackward0>)\n",
      "221 tensor(4359.5000, grad_fn=<SumBackward0>)\n",
      "222 tensor(4307.2886, grad_fn=<SumBackward0>)\n",
      "223 tensor(4255.8828, grad_fn=<SumBackward0>)\n",
      "224 tensor(4205.2310, grad_fn=<SumBackward0>)\n",
      "225 tensor(4155.4199, grad_fn=<SumBackward0>)\n",
      "226 tensor(4106.3589, grad_fn=<SumBackward0>)\n",
      "227 tensor(4058.0220, grad_fn=<SumBackward0>)\n",
      "228 tensor(4010.3818, grad_fn=<SumBackward0>)\n",
      "229 tensor(3963.4490, grad_fn=<SumBackward0>)\n",
      "230 tensor(3917.1846, grad_fn=<SumBackward0>)\n",
      "231 tensor(3871.6042, grad_fn=<SumBackward0>)\n",
      "232 tensor(3826.6934, grad_fn=<SumBackward0>)\n",
      "233 tensor(3782.4280, grad_fn=<SumBackward0>)\n",
      "234 tensor(3738.7898, grad_fn=<SumBackward0>)\n",
      "235 tensor(3695.7915, grad_fn=<SumBackward0>)\n",
      "236 tensor(3653.4009, grad_fn=<SumBackward0>)\n",
      "237 tensor(3611.6528, grad_fn=<SumBackward0>)\n",
      "238 tensor(3570.4688, grad_fn=<SumBackward0>)\n",
      "239 tensor(3529.8608, grad_fn=<SumBackward0>)\n",
      "240 tensor(3489.8264, grad_fn=<SumBackward0>)\n",
      "241 tensor(3450.3433, grad_fn=<SumBackward0>)\n",
      "242 tensor(3411.4185, grad_fn=<SumBackward0>)\n",
      "243 tensor(3373.0359, grad_fn=<SumBackward0>)\n",
      "244 tensor(3335.1958, grad_fn=<SumBackward0>)\n",
      "245 tensor(3297.8799, grad_fn=<SumBackward0>)\n",
      "246 tensor(3261.0781, grad_fn=<SumBackward0>)\n",
      "247 tensor(3224.7749, grad_fn=<SumBackward0>)\n",
      "248 tensor(3188.9712, grad_fn=<SumBackward0>)\n",
      "249 tensor(3153.6616, grad_fn=<SumBackward0>)\n",
      "250 tensor(3118.8635, grad_fn=<SumBackward0>)\n",
      "251 tensor(3084.5378, grad_fn=<SumBackward0>)\n",
      "252 tensor(3050.6543, grad_fn=<SumBackward0>)\n",
      "253 tensor(3017.2278, grad_fn=<SumBackward0>)\n",
      "254 tensor(2984.2568, grad_fn=<SumBackward0>)\n",
      "255 tensor(2951.7632, grad_fn=<SumBackward0>)\n",
      "256 tensor(2919.7075, grad_fn=<SumBackward0>)\n",
      "257 tensor(2888.0852, grad_fn=<SumBackward0>)\n",
      "258 tensor(2856.8740, grad_fn=<SumBackward0>)\n",
      "259 tensor(2826.0850, grad_fn=<SumBackward0>)\n",
      "260 tensor(2795.7146, grad_fn=<SumBackward0>)\n",
      "261 tensor(2765.7363, grad_fn=<SumBackward0>)\n",
      "262 tensor(2736.1560, grad_fn=<SumBackward0>)\n",
      "263 tensor(2706.9622, grad_fn=<SumBackward0>)\n",
      "264 tensor(2678.1890, grad_fn=<SumBackward0>)\n",
      "265 tensor(2649.7888, grad_fn=<SumBackward0>)\n",
      "266 tensor(2621.7529, grad_fn=<SumBackward0>)\n",
      "267 tensor(2594.0806, grad_fn=<SumBackward0>)\n",
      "268 tensor(2566.7678, grad_fn=<SumBackward0>)\n",
      "269 tensor(2539.8066, grad_fn=<SumBackward0>)\n",
      "270 tensor(2513.2119, grad_fn=<SumBackward0>)\n",
      "271 tensor(2486.9546, grad_fn=<SumBackward0>)\n",
      "272 tensor(2461.0356, grad_fn=<SumBackward0>)\n",
      "273 tensor(2435.4512, grad_fn=<SumBackward0>)\n",
      "274 tensor(2410.2070, grad_fn=<SumBackward0>)\n",
      "275 tensor(2385.2773, grad_fn=<SumBackward0>)\n",
      "276 tensor(2360.6538, grad_fn=<SumBackward0>)\n",
      "277 tensor(2336.3474, grad_fn=<SumBackward0>)\n",
      "278 tensor(2312.3674, grad_fn=<SumBackward0>)\n",
      "279 tensor(2288.6826, grad_fn=<SumBackward0>)\n",
      "280 tensor(2265.2886, grad_fn=<SumBackward0>)\n",
      "281 tensor(2242.1838, grad_fn=<SumBackward0>)\n",
      "282 tensor(2219.3748, grad_fn=<SumBackward0>)\n",
      "283 tensor(2196.8445, grad_fn=<SumBackward0>)\n",
      "284 tensor(2174.5957, grad_fn=<SumBackward0>)\n",
      "285 tensor(2152.6294, grad_fn=<SumBackward0>)\n",
      "286 tensor(2130.9385, grad_fn=<SumBackward0>)\n",
      "287 tensor(2109.5061, grad_fn=<SumBackward0>)\n",
      "288 tensor(2088.3433, grad_fn=<SumBackward0>)\n",
      "289 tensor(2067.4417, grad_fn=<SumBackward0>)\n",
      "290 tensor(2046.8054, grad_fn=<SumBackward0>)\n",
      "291 tensor(2026.4197, grad_fn=<SumBackward0>)\n",
      "292 tensor(2006.2904, grad_fn=<SumBackward0>)\n",
      "293 tensor(1986.4116, grad_fn=<SumBackward0>)\n",
      "294 tensor(1966.7715, grad_fn=<SumBackward0>)\n",
      "295 tensor(1947.3621, grad_fn=<SumBackward0>)\n",
      "296 tensor(1928.1852, grad_fn=<SumBackward0>)\n",
      "297 tensor(1909.2458, grad_fn=<SumBackward0>)\n",
      "298 tensor(1890.5315, grad_fn=<SumBackward0>)\n",
      "299 tensor(1872.0408, grad_fn=<SumBackward0>)\n",
      "300 tensor(1853.7805, grad_fn=<SumBackward0>)\n",
      "301 tensor(1835.7593, grad_fn=<SumBackward0>)\n",
      "302 tensor(1817.9568, grad_fn=<SumBackward0>)\n",
      "303 tensor(1800.3667, grad_fn=<SumBackward0>)\n",
      "304 tensor(1782.9818, grad_fn=<SumBackward0>)\n",
      "305 tensor(1765.8069, grad_fn=<SumBackward0>)\n",
      "306 tensor(1748.8403, grad_fn=<SumBackward0>)\n",
      "307 tensor(1732.0835, grad_fn=<SumBackward0>)\n",
      "308 tensor(1715.5116, grad_fn=<SumBackward0>)\n",
      "309 tensor(1699.1344, grad_fn=<SumBackward0>)\n",
      "310 tensor(1682.9517, grad_fn=<SumBackward0>)\n",
      "311 tensor(1666.9674, grad_fn=<SumBackward0>)\n",
      "312 tensor(1651.1614, grad_fn=<SumBackward0>)\n",
      "313 tensor(1635.5359, grad_fn=<SumBackward0>)\n",
      "314 tensor(1620.0979, grad_fn=<SumBackward0>)\n",
      "315 tensor(1604.8392, grad_fn=<SumBackward0>)\n",
      "316 tensor(1589.7537, grad_fn=<SumBackward0>)\n",
      "317 tensor(1574.8385, grad_fn=<SumBackward0>)\n",
      "318 tensor(1560.1031, grad_fn=<SumBackward0>)\n",
      "319 tensor(1545.5388, grad_fn=<SumBackward0>)\n",
      "320 tensor(1531.1393, grad_fn=<SumBackward0>)\n",
      "321 tensor(1516.9180, grad_fn=<SumBackward0>)\n",
      "322 tensor(1502.8510, grad_fn=<SumBackward0>)\n",
      "323 tensor(1488.9407, grad_fn=<SumBackward0>)\n",
      "324 tensor(1475.1912, grad_fn=<SumBackward0>)\n",
      "325 tensor(1461.5968, grad_fn=<SumBackward0>)\n",
      "326 tensor(1448.1567, grad_fn=<SumBackward0>)\n",
      "327 tensor(1434.8699, grad_fn=<SumBackward0>)\n",
      "328 tensor(1421.7344, grad_fn=<SumBackward0>)\n",
      "329 tensor(1408.7512, grad_fn=<SumBackward0>)\n",
      "330 tensor(1395.9141, grad_fn=<SumBackward0>)\n",
      "331 tensor(1383.2057, grad_fn=<SumBackward0>)\n",
      "332 tensor(1370.6421, grad_fn=<SumBackward0>)\n",
      "333 tensor(1358.2213, grad_fn=<SumBackward0>)\n",
      "334 tensor(1345.9386, grad_fn=<SumBackward0>)\n",
      "335 tensor(1333.8013, grad_fn=<SumBackward0>)\n",
      "336 tensor(1321.8025, grad_fn=<SumBackward0>)\n",
      "337 tensor(1309.9290, grad_fn=<SumBackward0>)\n",
      "338 tensor(1298.1880, grad_fn=<SumBackward0>)\n",
      "339 tensor(1286.5791, grad_fn=<SumBackward0>)\n",
      "340 tensor(1275.0959, grad_fn=<SumBackward0>)\n",
      "341 tensor(1263.7397, grad_fn=<SumBackward0>)\n",
      "342 tensor(1252.5103, grad_fn=<SumBackward0>)\n",
      "343 tensor(1241.4055, grad_fn=<SumBackward0>)\n",
      "344 tensor(1230.4238, grad_fn=<SumBackward0>)\n",
      "345 tensor(1219.5629, grad_fn=<SumBackward0>)\n",
      "346 tensor(1208.8224, grad_fn=<SumBackward0>)\n",
      "347 tensor(1198.2020, grad_fn=<SumBackward0>)\n",
      "348 tensor(1187.6930, grad_fn=<SumBackward0>)\n",
      "349 tensor(1177.3066, grad_fn=<SumBackward0>)\n",
      "350 tensor(1167.0310, grad_fn=<SumBackward0>)\n",
      "351 tensor(1156.8683, grad_fn=<SumBackward0>)\n",
      "352 tensor(1146.8099, grad_fn=<SumBackward0>)\n",
      "353 tensor(1136.8635, grad_fn=<SumBackward0>)\n",
      "354 tensor(1127.0205, grad_fn=<SumBackward0>)\n",
      "355 tensor(1117.2872, grad_fn=<SumBackward0>)\n",
      "356 tensor(1107.6592, grad_fn=<SumBackward0>)\n",
      "357 tensor(1098.1332, grad_fn=<SumBackward0>)\n",
      "358 tensor(1088.7145, grad_fn=<SumBackward0>)\n",
      "359 tensor(1079.3964, grad_fn=<SumBackward0>)\n",
      "360 tensor(1070.1768, grad_fn=<SumBackward0>)\n",
      "361 tensor(1061.0569, grad_fn=<SumBackward0>)\n",
      "362 tensor(1052.0358, grad_fn=<SumBackward0>)\n",
      "363 tensor(1043.1086, grad_fn=<SumBackward0>)\n",
      "364 tensor(1034.2765, grad_fn=<SumBackward0>)\n",
      "365 tensor(1025.5511, grad_fn=<SumBackward0>)\n",
      "366 tensor(1016.9288, grad_fn=<SumBackward0>)\n",
      "367 tensor(1008.3979, grad_fn=<SumBackward0>)\n",
      "368 tensor(999.9520, grad_fn=<SumBackward0>)\n",
      "369 tensor(991.5992, grad_fn=<SumBackward0>)\n",
      "370 tensor(983.3325, grad_fn=<SumBackward0>)\n",
      "371 tensor(975.1524, grad_fn=<SumBackward0>)\n",
      "372 tensor(967.0576, grad_fn=<SumBackward0>)\n",
      "373 tensor(959.0476, grad_fn=<SumBackward0>)\n",
      "374 tensor(951.1232, grad_fn=<SumBackward0>)\n",
      "375 tensor(943.2794, grad_fn=<SumBackward0>)\n",
      "376 tensor(935.5186, grad_fn=<SumBackward0>)\n",
      "377 tensor(927.8387, grad_fn=<SumBackward0>)\n",
      "378 tensor(920.2404, grad_fn=<SumBackward0>)\n",
      "379 tensor(912.7194, grad_fn=<SumBackward0>)\n",
      "380 tensor(905.2795, grad_fn=<SumBackward0>)\n",
      "381 tensor(897.9136, grad_fn=<SumBackward0>)\n",
      "382 tensor(890.6224, grad_fn=<SumBackward0>)\n",
      "383 tensor(883.4072, grad_fn=<SumBackward0>)\n",
      "384 tensor(876.2676, grad_fn=<SumBackward0>)\n",
      "385 tensor(869.1973, grad_fn=<SumBackward0>)\n",
      "386 tensor(862.2045, grad_fn=<SumBackward0>)\n",
      "387 tensor(855.2828, grad_fn=<SumBackward0>)\n",
      "388 tensor(848.4291, grad_fn=<SumBackward0>)\n",
      "389 tensor(841.6453, grad_fn=<SumBackward0>)\n",
      "390 tensor(834.9295, grad_fn=<SumBackward0>)\n",
      "391 tensor(828.2817, grad_fn=<SumBackward0>)\n",
      "392 tensor(821.7003, grad_fn=<SumBackward0>)\n",
      "393 tensor(815.1890, grad_fn=<SumBackward0>)\n",
      "394 tensor(808.7483, grad_fn=<SumBackward0>)\n",
      "395 tensor(802.3669, grad_fn=<SumBackward0>)\n",
      "396 tensor(796.0493, grad_fn=<SumBackward0>)\n",
      "397 tensor(789.7947, grad_fn=<SumBackward0>)\n",
      "398 tensor(783.6027, grad_fn=<SumBackward0>)\n",
      "399 tensor(777.4736, grad_fn=<SumBackward0>)\n",
      "400 tensor(771.4075, grad_fn=<SumBackward0>)\n",
      "401 tensor(765.4006, grad_fn=<SumBackward0>)\n",
      "402 tensor(759.4529, grad_fn=<SumBackward0>)\n",
      "403 tensor(753.5669, grad_fn=<SumBackward0>)\n",
      "404 tensor(747.7482, grad_fn=<SumBackward0>)\n",
      "405 tensor(741.9891, grad_fn=<SumBackward0>)\n",
      "406 tensor(736.2888, grad_fn=<SumBackward0>)\n",
      "407 tensor(730.6462, grad_fn=<SumBackward0>)\n",
      "408 tensor(725.0613, grad_fn=<SumBackward0>)\n",
      "409 tensor(719.5338, grad_fn=<SumBackward0>)\n",
      "410 tensor(714.0571, grad_fn=<SumBackward0>)\n",
      "411 tensor(708.6342, grad_fn=<SumBackward0>)\n",
      "412 tensor(703.2642, grad_fn=<SumBackward0>)\n",
      "413 tensor(697.9511, grad_fn=<SumBackward0>)\n",
      "414 tensor(692.6859, grad_fn=<SumBackward0>)\n",
      "415 tensor(687.4732, grad_fn=<SumBackward0>)\n",
      "416 tensor(682.3115, grad_fn=<SumBackward0>)\n",
      "417 tensor(677.2028, grad_fn=<SumBackward0>)\n",
      "418 tensor(672.1410, grad_fn=<SumBackward0>)\n",
      "419 tensor(667.1309, grad_fn=<SumBackward0>)\n",
      "420 tensor(662.1700, grad_fn=<SumBackward0>)\n",
      "421 tensor(657.2560, grad_fn=<SumBackward0>)\n",
      "422 tensor(652.3896, grad_fn=<SumBackward0>)\n",
      "423 tensor(647.5758, grad_fn=<SumBackward0>)\n",
      "424 tensor(642.8062, grad_fn=<SumBackward0>)\n",
      "425 tensor(638.0801, grad_fn=<SumBackward0>)\n",
      "426 tensor(633.3987, grad_fn=<SumBackward0>)\n",
      "427 tensor(628.7652, grad_fn=<SumBackward0>)\n",
      "428 tensor(624.1747, grad_fn=<SumBackward0>)\n",
      "429 tensor(619.6268, grad_fn=<SumBackward0>)\n",
      "430 tensor(615.1243, grad_fn=<SumBackward0>)\n",
      "431 tensor(610.6642, grad_fn=<SumBackward0>)\n",
      "432 tensor(606.2463, grad_fn=<SumBackward0>)\n",
      "433 tensor(601.8723, grad_fn=<SumBackward0>)\n",
      "434 tensor(597.5405, grad_fn=<SumBackward0>)\n",
      "435 tensor(593.2493, grad_fn=<SumBackward0>)\n",
      "436 tensor(588.9994, grad_fn=<SumBackward0>)\n",
      "437 tensor(584.7905, grad_fn=<SumBackward0>)\n",
      "438 tensor(580.6233, grad_fn=<SumBackward0>)\n",
      "439 tensor(576.4910, grad_fn=<SumBackward0>)\n",
      "440 tensor(572.3991, grad_fn=<SumBackward0>)\n",
      "441 tensor(568.3472, grad_fn=<SumBackward0>)\n",
      "442 tensor(564.3329, grad_fn=<SumBackward0>)\n",
      "443 tensor(560.3560, grad_fn=<SumBackward0>)\n",
      "444 tensor(556.4161, grad_fn=<SumBackward0>)\n",
      "445 tensor(552.5146, grad_fn=<SumBackward0>)\n",
      "446 tensor(548.6490, grad_fn=<SumBackward0>)\n",
      "447 tensor(544.8192, grad_fn=<SumBackward0>)\n",
      "448 tensor(541.0273, grad_fn=<SumBackward0>)\n",
      "449 tensor(537.2681, grad_fn=<SumBackward0>)\n",
      "450 tensor(533.5443, grad_fn=<SumBackward0>)\n",
      "451 tensor(529.8598, grad_fn=<SumBackward0>)\n",
      "452 tensor(526.2087, grad_fn=<SumBackward0>)\n",
      "453 tensor(522.5887, grad_fn=<SumBackward0>)\n",
      "454 tensor(519.0011, grad_fn=<SumBackward0>)\n",
      "455 tensor(515.4474, grad_fn=<SumBackward0>)\n",
      "456 tensor(511.9277, grad_fn=<SumBackward0>)\n",
      "457 tensor(508.4402, grad_fn=<SumBackward0>)\n",
      "458 tensor(504.9850, grad_fn=<SumBackward0>)\n",
      "459 tensor(501.5616, grad_fn=<SumBackward0>)\n",
      "460 tensor(498.1703, grad_fn=<SumBackward0>)\n",
      "461 tensor(494.8119, grad_fn=<SumBackward0>)\n",
      "462 tensor(491.4822, grad_fn=<SumBackward0>)\n",
      "463 tensor(488.1833, grad_fn=<SumBackward0>)\n",
      "464 tensor(484.9147, grad_fn=<SumBackward0>)\n",
      "465 tensor(481.6788, grad_fn=<SumBackward0>)\n",
      "466 tensor(478.4703, grad_fn=<SumBackward0>)\n",
      "467 tensor(475.2909, grad_fn=<SumBackward0>)\n",
      "468 tensor(472.1410, grad_fn=<SumBackward0>)\n",
      "469 tensor(469.0192, grad_fn=<SumBackward0>)\n",
      "470 tensor(465.9259, grad_fn=<SumBackward0>)\n",
      "471 tensor(462.8591, grad_fn=<SumBackward0>)\n",
      "472 tensor(459.8222, grad_fn=<SumBackward0>)\n",
      "473 tensor(456.8118, grad_fn=<SumBackward0>)\n",
      "474 tensor(453.8299, grad_fn=<SumBackward0>)\n",
      "475 tensor(450.8737, grad_fn=<SumBackward0>)\n",
      "476 tensor(447.9456, grad_fn=<SumBackward0>)\n",
      "477 tensor(445.0412, grad_fn=<SumBackward0>)\n",
      "478 tensor(442.1641, grad_fn=<SumBackward0>)\n",
      "479 tensor(439.3147, grad_fn=<SumBackward0>)\n",
      "480 tensor(436.4901, grad_fn=<SumBackward0>)\n",
      "481 tensor(433.6896, grad_fn=<SumBackward0>)\n",
      "482 tensor(430.9147, grad_fn=<SumBackward0>)\n",
      "483 tensor(428.1644, grad_fn=<SumBackward0>)\n",
      "484 tensor(425.4389, grad_fn=<SumBackward0>)\n",
      "485 tensor(422.7373, grad_fn=<SumBackward0>)\n",
      "486 tensor(420.0586, grad_fn=<SumBackward0>)\n",
      "487 tensor(417.4052, grad_fn=<SumBackward0>)\n",
      "488 tensor(414.7752, grad_fn=<SumBackward0>)\n",
      "489 tensor(412.1676, grad_fn=<SumBackward0>)\n",
      "490 tensor(409.5837, grad_fn=<SumBackward0>)\n",
      "491 tensor(407.0236, grad_fn=<SumBackward0>)\n",
      "492 tensor(404.4857, grad_fn=<SumBackward0>)\n",
      "493 tensor(401.9724, grad_fn=<SumBackward0>)\n",
      "494 tensor(399.4774, grad_fn=<SumBackward0>)\n",
      "495 tensor(397.0067, grad_fn=<SumBackward0>)\n",
      "496 tensor(394.5550, grad_fn=<SumBackward0>)\n",
      "497 tensor(392.1263, grad_fn=<SumBackward0>)\n",
      "498 tensor(389.7202, grad_fn=<SumBackward0>)\n",
      "499 tensor(387.3339, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Autograd Version\n",
    "import torch\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# init the weights\n",
    "w1 = torch.randn(D_in, D_h, device=device, requires_grad=True)\n",
    "w2 = torch.randn(D_h, D_out, device=device, requires_grad=True)\n",
    "\n",
    "# training\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    # calculate the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "    # use autograd to do the back-propagation\n",
    "    loss.backward()\n",
    "    # update the weights, prevent torch do autograd for this part\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # zero grads\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
