{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding: Machine Translation with Transformer\n",
    "\n",
    " - Dataset: wmt-17, en-zh, select 1m high-quality pairs of data\n",
    " - Model: Seq2seq with Transformer (Attention is all you need)\n",
    " - GPU: 4090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset Size:  25134743\n",
      "Filtered Dataset Size:  1141860\n",
      "Dataset Size:  1141860\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Zambia (7)\n",
      "赞比亚(7)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "15:00 to 18:00 Informal consultations (closed) Conference Room 5 (NLB)\n",
      "下午3:00－6:00 非正式磋商(闭门会议) 第5会议室(北草坪会议大楼)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spain\n",
      "西班牙\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mr. Robert Morrison\n",
      "Robert Morrison先生 加拿大自然资源部\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This satisfied the kids, but not the husband.\n",
      "\"孩子们得到了满意的答案, 但她的丈夫却没有。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shutaro Omura (Japan)\n",
      "Shutaro Omura（日本）\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\"Oh,\" she says, \"what am I THINKING about!\n",
      "——”    “哦，”她说，“你看我在想些什么啊！\n",
      "----------------------------------------------------------------------------------------------------\n",
      "30 June 2005\n",
      "2005年6月30日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "11 July-5 August 2005\n",
      "2005年7月11日-8月5日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "May 2011\n",
      "2011年5月\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download the data & select 5m high-quality pairs\n",
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# load full wmt-17 en-zh dataset\n",
    "full_dataset = load_dataset(\"wmt/wmt17\", \"zh-en\", split=\"train\")\n",
    "\n",
    "# Length & Ratio filter\n",
    "def is_high_quality(x):\n",
    "    en = x[\"translation\"][\"en\"]\n",
    "    zh = x[\"translation\"][\"zh\"]\n",
    "    if not en or not zh:\n",
    "        return False\n",
    "    if len(en) < 3 or len(zh) < 3:\n",
    "        return False\n",
    "    if len(en) > 100 or len(zh) > 100:\n",
    "        return False\n",
    "    ratio = len(en) / len(zh)\n",
    "    if ratio < 0.5 or ratio > 2:\n",
    "        return False\n",
    "    if not re.search(r'[\\u4e00-\\u9fff]', zh):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_dataset = full_dataset.filter(is_high_quality, num_proc=10)\n",
    "\n",
    "dataset = filtered_dataset.select(range(min(5_000_000, len(filtered_dataset))))\n",
    "\n",
    "print(\"Full Dataset Size: \", len(full_dataset))\n",
    "print(\"Filtered Dataset Size: \", len(filtered_dataset))\n",
    "print(\"Dataset Size: \", len(dataset))\n",
    "\n",
    "# print 10 samples\n",
    "sample = dataset.shuffle(seed=42).select(range(10))\n",
    "print(\"-\"*100)\n",
    "for i in sample:\n",
    "    print(i[\"translation\"][\"en\"])\n",
    "    print(i[\"translation\"][\"zh\"])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for en: 30522\n",
      "Vocab size for zh: 21128\n",
      "Train dataset size: 1084767\n",
      "Validation dataset size: 57093\n",
      "Train DataLoader: 8475 batches\n",
      "Validation DataLoader: 447 batches\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 128\n",
      "Source example: Switzerland\n",
      "Source tokens: tensor([ 101, 5288,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "Target example: 瑞士 460.9 1.77\n",
      "Target tokens: tensor([  101,  4448,  1894, 10685,   119,   130,   122,   119,  8411,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 128\n",
      "Source example: 74 and A/60/640/Add.1, para. 6\n",
      "Source tokens: tensor([  101,  6356,  1998,  1037,  1013,  3438,  1013, 19714,  1013,  5587,\n",
      "         1012,  1015,  1010, 11498,  1012,  1020,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Target example: (A/60/640/Add.1，第6段)\n",
      "Target tokens: tensor([  101,   113,   100,   120,  8183,   120, 11410,   120,   100,   119,\n",
      "          122,  8024,  5018,   127,  3667,   114,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Dataset and DataLoader for training\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer_en, tokenizer_zh, max_length=100):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset wrapper for HuggingFace translation dataset\n",
    "        \n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset with translation pairs\n",
    "            tokenizer_en: English tokenizer (optional, can be added later)\n",
    "            tokenizer_zh: Chinese tokenizer (optional, can be added later)\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_zh = tokenizer_zh\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        en_text = item[\"translation\"][\"en\"]\n",
    "        zh_text = item[\"translation\"][\"zh\"]\n",
    "\n",
    "        en_tokens = self.tokenizer_en(en_text, \n",
    "                                        max_length=self.max_length, \n",
    "                                        padding='max_length', \n",
    "                                        truncation=True, \n",
    "                                        return_tensors='pt')\n",
    "            \n",
    "        zh_tokens = self.tokenizer_zh(zh_text, \n",
    "                                        max_length=self.max_length, \n",
    "                                        padding='max_length', \n",
    "                                        truncation=True, \n",
    "                                        return_tensors='pt')\n",
    "            \n",
    "        return {\n",
    "                'source_ids': en_tokens['input_ids'].squeeze(),\n",
    "                'target_ids': zh_tokens['input_ids'].squeeze(),\n",
    "                'source_text': en_text,\n",
    "                'target_text': zh_text\n",
    "        }\n",
    "\n",
    "def create_dataloaders(dataset, batch_size=128, num_workers=12, train_split=0.95):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders from HuggingFace dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with translation pairs\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        train_split: Fraction of data to use for training\n",
    "    \n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader, train_dataset, val_dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split dataset into train and validation\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    \n",
    "    # Create indices for splitting\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices, \n",
    "                                                train_size=train_size, \n",
    "                                                random_state=42)\n",
    "    \n",
    "    # Create train and validation datasets\n",
    "    train_dataset_hf = dataset.select(train_indices)\n",
    "    val_dataset_hf = dataset.select(val_indices)\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "    # get vocab sizes\n",
    "    vocab_size_en = tokenizer_en.vocab_size\n",
    "    vocab_size_zh = tokenizer_zh.vocab_size\n",
    "\n",
    "    print(f\"Vocab size for en: {vocab_size_en}\")\n",
    "    print(f\"Vocab size for zh: {vocab_size_zh}\")\n",
    "\n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = TranslationDataset(train_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "    val_dataset = TranslationDataset(val_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Train DataLoader: {len(train_dataloader)} batches\")\n",
    "    print(f\"Validation DataLoader: {len(val_dataloader)} batches\")\n",
    "    \n",
    "    return train_dataloader, val_dataloader, vocab_size_en, vocab_size_zh\n",
    "\n",
    "def test_dataloader(dataloader):\n",
    "    \"\"\"Test the DataLoader by printing a sample batch\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Sample batch from DataLoader:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        print(f\"Batch size: {len(batch['source_text'])}\")\n",
    "        print(f\"Source example: {batch['source_text'][0]}\")\n",
    "        print(f\"Source tokens: {batch['source_ids'][0]}\")\n",
    "        print(f\"Target example: {batch['target_text'][0]}\")\n",
    "        print(f\"Target tokens: {batch['target_ids'][0]}\")\n",
    "        break\n",
    "\n",
    "train_dataloader, val_dataloader, encoder_vocab_size, decoder_vocab_size = create_dataloaders(dataset)\n",
    "test_dataloader(train_dataloader)\n",
    "test_dataloader(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Transformer Model Architecture ===\n",
      "Device: cuda\n",
      "Encoder Vocabulary Size: 30,522\n",
      "Decoder Vocabulary Size: 21,128\n",
      "Model Dimension: 512\n",
      "Number of Heads: 8\n",
      "Encoder Layers: 2\n",
      "Decoder Layers: 2\n",
      "Feed Forward Dimension: 2048\n",
      "Dropout Rate: 0.1\n",
      "\n",
      "Total Parameters: 51,996,296\n",
      "Trainable Parameters: 51,996,296\n",
      "Model Size: 198.35 MB\n",
      "\n",
      "=== Model Architecture ===\n",
      "Transformer(\n",
      "  (encoder_embedding): Embedding(30522, 512, padding_idx=0)\n",
      "  (decoder_embedding): Embedding(21128, 512, padding_idx=0)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-1): 2 x TransformerEncoderLayer(\n",
      "      (self_attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-1): 2 x TransformerDecoderLayer(\n",
      "      (self_attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (cross_attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=512, out_features=21128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "=== Testing Model Forward Pass ===\n",
      "Source shape: torch.Size([128, 100])\n",
      "Target shape: torch.Size([128, 100])\n",
      "Output shape: torch.Size([128, 99, 21128])\n",
      "Expected shape: [batch_size, target_len-1, decoder_vocab_size]\n",
      "Actual shape: [128, 99, 21128]\n",
      "\n",
      "=== Testing Model Generation ===\n",
      "Generated sequence shape: torch.Size([2, 50])\n",
      "Generated tokens (first sample): [16792, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650, 11650]...\n"
     ]
    }
   ],
   "source": [
    "# Define the Transformer model for Machine Translation\n",
    "# Following \"Attention is All You Need\" architecture\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear transformations and reshape\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        self_attn_output = self.self_attention(x, x, x, self_attn_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        \n",
    "        # Cross-attention with residual connection\n",
    "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, cross_attn_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size, d_model=512, num_heads=8, \n",
    "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, dropout=0.1, max_len=100):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        \n",
    "        # Embeddings\n",
    "        self.encoder_embedding = nn.Embedding(encoder_vocab_size, d_model, padding_idx=0)\n",
    "        self.decoder_embedding = nn.Embedding(decoder_vocab_size, d_model, padding_idx=0)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, decoder_vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def create_padding_mask(self, x, pad_idx=0):\n",
    "        return (x != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        return mask == 0\n",
    "    \n",
    "    def encode(self, src, src_mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        src_emb = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.positional_encoding(src_emb.transpose(0, 1)).transpose(0, 1)\n",
    "        src_emb = self.dropout(src_emb)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        encoder_output = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output, src_mask)\n",
    "        \n",
    "        return encoder_output\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, tgt_mask=None, src_mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        tgt_emb = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb.transpose(0, 1)).transpose(0, 1)\n",
    "        tgt_emb = self.dropout(tgt_emb)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        decoder_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output, encoder_output, tgt_mask, src_mask)\n",
    "        \n",
    "        return decoder_output\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        # Create masks\n",
    "        src_mask = self.create_padding_mask(src)\n",
    "        tgt_mask = self.create_padding_mask(tgt) & self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)\n",
    "        \n",
    "        # Encode\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output = self.decode(tgt, encoder_output, tgt_mask, src_mask)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_projection(decoder_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate(self, src, max_length=100, bos_token=101, eos_token=102, pad_token=0):\n",
    "        \"\"\"Generate translation using greedy decoding\"\"\"\n",
    "        self.eval()\n",
    "        batch_size = src.size(0)\n",
    "        device = src.device\n",
    "        \n",
    "        # Encode source\n",
    "        src_mask = self.create_padding_mask(src)\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        \n",
    "        # Initialize decoder input with BOS token\n",
    "        decoder_input = torch.full((batch_size, 1), bos_token, device=device)\n",
    "        \n",
    "        generated_sequences = []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Create target mask\n",
    "            tgt_mask = self.create_padding_mask(decoder_input) & \\\n",
    "                      self.create_look_ahead_mask(decoder_input.size(1)).to(device)\n",
    "            \n",
    "            # Decode\n",
    "            decoder_output = self.decode(decoder_input, encoder_output, tgt_mask, src_mask)\n",
    "            \n",
    "            # Get next token probabilities\n",
    "            next_token_logits = self.output_projection(decoder_output[:, -1, :])\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Append to decoder input\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "            \n",
    "            # Check if all sequences have generated EOS token\n",
    "            if torch.all(next_token.squeeze() == eos_token):\n",
    "                break\n",
    "        \n",
    "        # Remove BOS token from generated sequences\n",
    "        generated_sequences = decoder_input[:, 1:]\n",
    "        \n",
    "        return generated_sequences\n",
    "\n",
    "# Model configuration based on your dataset\n",
    "model_config = {\n",
    "    'encoder_vocab_size': encoder_vocab_size,  # 30522 (English BERT)\n",
    "    'decoder_vocab_size': decoder_vocab_size,  # 21128 (Chinese BERT)\n",
    "    'd_model': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_encoder_layers': 2,\n",
    "    'num_decoder_layers': 2,\n",
    "    'd_ff': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'max_len': 100\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = Transformer(**model_config)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"=== Transformer Model Architecture ===\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Encoder Vocabulary Size: {model_config['encoder_vocab_size']:,}\")\n",
    "print(f\"Decoder Vocabulary Size: {model_config['decoder_vocab_size']:,}\")\n",
    "print(f\"Model Dimension: {model_config['d_model']}\")\n",
    "print(f\"Number of Heads: {model_config['num_heads']}\")\n",
    "print(f\"Encoder Layers: {model_config['num_encoder_layers']}\")\n",
    "print(f\"Decoder Layers: {model_config['num_decoder_layers']}\")\n",
    "print(f\"Feed Forward Dimension: {model_config['d_ff']}\")\n",
    "print(f\"Dropout Rate: {model_config['dropout']}\")\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Model Size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"\\n=== Model Architecture ===\")\n",
    "print(model)\n",
    "\n",
    "# Test the model with a sample batch\n",
    "print(f\"\\n=== Testing Model Forward Pass ===\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "source_ids = sample_batch['source_ids'].to(device)\n",
    "target_ids = sample_batch['target_ids'].to(device)\n",
    "\n",
    "print(f\"Source shape: {source_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.train()\n",
    "decoder_input = target_ids[:, :-1]  # Remove last token for teacher forcing\n",
    "outputs = model(source_ids, decoder_input)\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "print(f\"Expected shape: [batch_size, target_len-1, decoder_vocab_size]\")\n",
    "print(f\"Actual shape: [{outputs.shape[0]}, {outputs.shape[1]}, {outputs.shape[2]}]\")\n",
    "\n",
    "# Test generation\n",
    "print(f\"\\n=== Testing Model Generation ===\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(source_ids[:2], max_length=50)  # Generate for first 2 samples\n",
    "    print(f\"Generated sequence shape: {generated.shape}\")\n",
    "    print(f\"Generated tokens (first sample): {generated[0].tolist()[:20]}...\")  # Show first 20 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "=== Starting Training for 10 Epochs ===\n",
      "Device: cuda\n",
      "Train batches: 8475\n",
      "Validation batches: 447\n",
      "Model parameters: 51,996,296\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 264\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# Start training for 10 epochs\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m history = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_seq2seq_attention_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlotting training history...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, num_epochs, save_path)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    156\u001b[39m val_loss = \u001b[38;5;28mself\u001b[39m.validate_epoch()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     78\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Update metrics\u001b[39;00m\n\u001b[32m     84\u001b[39m epoch_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch/lib/python3.12/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch/lib/python3.12/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch/lib/python3.12/site-packages/torch/optim/adam.py:595\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m grad_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m found_inf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m differentiable, \u001b[33m\"\u001b[39m\u001b[33m_foreach ops don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt support autograd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m grouped_tensors = \u001b[43mOptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_group_tensors_by_device_and_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[list-item]\u001b[39;49;00m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# We only shuffle around the beta when it is a Tensor and on CUDA, otherwise, we prefer\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# treating it as a scalar.\u001b[39;00m\n\u001b[32m    601\u001b[39m beta1_dict: Optional[DeviceDict] = (  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    602\u001b[39m     {beta1.device: beta1}\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(beta1, Tensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(beta1.device) != \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    605\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch/lib/python3.12/site-packages/torch/optim/optimizer.py:514\u001b[39m, in \u001b[36mOptimizer._group_tensors_by_device_and_dtype\u001b[39m\u001b[34m(tensorlistlist, with_indices)\u001b[39m\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m): (tensorlistlist, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tensorlistlist[\u001b[32m0\u001b[39m]))))}\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_group_tensors_by_device_and_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensorlistlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch/lib/python3.12/site-packages/torch/utils/_foreach_utils.py:37\u001b[39m, in \u001b[36m_group_tensors_by_device_and_dtype\u001b[39m\u001b[34m(tensorlistlist, with_indices)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;129m@no_grad\u001b[39m()\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_group_tensors_by_device_and_dtype\u001b[39m(\n\u001b[32m     34\u001b[39m     tensorlistlist: TensorListList,\n\u001b[32m     35\u001b[39m     with_indices: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     36\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[torch.device, torch.dtype], \u001b[38;5;28mtuple\u001b[39m[TensorListList, Indices]]:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_group_tensors_by_device_and_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensorlistlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the model with comprehensive training loop\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for Seq2Seq machine translation model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_dataloader, val_dataloader, device, \n",
    "                 learning_rate=1e-3, weight_decay=1e-5):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.device = device\n",
    "        \n",
    "        # Loss function - ignore padding tokens (index 0)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(model.parameters(), \n",
    "                                  lr=learning_rate, \n",
    "                                  weight_decay=weight_decay)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=2\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = defaultdict(list)\n",
    "        \n",
    "    def calculate_loss(self, outputs, targets, target_lengths=None):\n",
    "        \"\"\"Calculate loss for the batch\"\"\"\n",
    "        # Reshape for loss calculation (use reshape instead of view for non-contiguous tensors)\n",
    "        outputs_flat = outputs.reshape(-1, outputs.size(-1))\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.criterion(outputs_flat, targets_flat)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(self.train_dataloader, desc=\"Training\", leave=False)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            source_ids = batch['source_ids'].to(self.device)\n",
    "            target_ids = batch['target_ids'].to(self.device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with teacher forcing\n",
    "            decoder_input = target_ids[:, :-1]  # Remove last token\n",
    "            decoder_targets = target_ids[:, 1:]  # Remove first token (BOS)\n",
    "            \n",
    "            outputs = self.model(source_ids, decoder_input)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.calculate_loss(outputs, decoder_targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{epoch_loss / num_batches:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if batch_idx % 100 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return epoch_loss / num_batches\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(self.val_dataloader, desc=\"Validation\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                source_ids = batch['source_ids'].to(self.device)\n",
    "                target_ids = batch['target_ids'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                decoder_input = target_ids[:, :-1]\n",
    "                decoder_targets = target_ids[:, 1:]\n",
    "                \n",
    "                outputs = self.model(source_ids, decoder_input)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = self.calculate_loss(outputs, decoder_targets)\n",
    "                \n",
    "                # Update metrics\n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'Val Loss': f'{loss.item():.4f}',\n",
    "                    'Avg Val Loss': f'{epoch_loss / num_batches:.4f}'\n",
    "                })\n",
    "        \n",
    "        return epoch_loss / num_batches\n",
    "    \n",
    "    def train(self, num_epochs=10, save_path=None):\n",
    "        \"\"\"Train the model for specified number of epochs\"\"\"\n",
    "        print(f\"=== Starting Training for {num_epochs} Epochs ===\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Train batches: {len(self.train_dataloader)}\")\n",
    "        print(f\"Validation batches: {len(self.val_dataloader)}\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss = self.validate_epoch()\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Calculate epoch time\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            \n",
    "            # Store history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['epoch_time'].append(epoch_time)\n",
    "            self.history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                print(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "                \n",
    "                if save_path:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'model_config': model_config\n",
    "                    }, save_path)\n",
    "                    print(f\"Model saved to {save_path}\")\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Training summary\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training completed in {total_time:.2f}s ({total_time/60:.2f} min)\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "        print(f\"Final train loss: {self.history['train_loss'][-1]:.4f}\")\n",
    "        print(f\"Final val loss: {self.history['val_loss'][-1]:.4f}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if not self.history['train_loss']:\n",
    "            print(\"No training history to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[0, 0].plot(self.history['train_loss'], label='Train Loss', color='blue')\n",
    "        axes[0, 0].plot(self.history['val_loss'], label='Validation Loss', color='red')\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot learning rate\n",
    "        axes[0, 1].plot(self.history['learning_rate'], color='green')\n",
    "        axes[0, 1].set_title('Learning Rate Schedule')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Learning Rate')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot epoch time\n",
    "        axes[1, 0].plot(self.history['epoch_time'], color='orange')\n",
    "        axes[1, 0].set_title('Epoch Training Time')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Time (seconds)')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot loss comparison\n",
    "        x = range(len(self.history['train_loss']))\n",
    "        axes[1, 1].fill_between(x, self.history['train_loss'], alpha=0.3, color='blue', label='Train Loss')\n",
    "        axes[1, 1].fill_between(x, self.history['val_loss'], alpha=0.3, color='red', label='Val Loss')\n",
    "        axes[1, 1].plot(self.history['train_loss'], color='blue', linewidth=2)\n",
    "        axes[1, 1].plot(self.history['val_loss'], color='red', linewidth=2)\n",
    "        axes[1, 1].set_title('Loss Comparison')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=device,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Start training for 10 epochs\n",
    "print(\"Starting training...\")\n",
    "history = trainer.train(num_epochs=10, save_path=\"best_seq2seq_attention_model.pth\")\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nPlotting training history...\")\n",
    "trainer.plot_training_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Transformer translation tester...\n",
      "=== Transformer Translation Tester Initialized ===\n",
      "Device: cuda\n",
      "English tokenizer vocab size: 30522\n",
      "Chinese tokenizer vocab size: 21128\n",
      "Max sequence length: 100\n",
      "\n",
      "================================================================================\n",
      "WARNING: Model appears to be untrained!\n",
      "================================================================================\n",
      "The model has not been trained yet, so translations will be random.\n",
      "To get meaningful results, you should train the model first.\n",
      "You can still test the translation pipeline, but expect poor quality.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING SIMPLE EXAMPLES (Greedy Decoding)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "MACHINE TRANSLATION TESTING RESULTS\n",
      "================================================================================\n",
      "Model: Transformer (Attention is All You Need)\n",
      "Decoding: Greedy\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test 1/3\n",
      "----------------------------------------\n",
      "English:  Hello\n",
      "Expected: 你好\n",
      "Generated: [Empty translation]\n",
      "Time: 1.533s\n",
      "BLEU Score: 0.0000\n",
      "\n",
      "Test 2/3\n",
      "----------------------------------------\n",
      "English:  Good morning\n",
      "Expected: 早上好\n",
      "Generated: [Empty translation]\n",
      "Time: 0.004s\n",
      "BLEU Score: 0.0000\n",
      "\n",
      "Test 3/3\n",
      "----------------------------------------\n",
      "English:  Thank you\n",
      "Expected: 谢谢\n",
      "Generated: [Empty translation]\n",
      "Time: 0.003s\n",
      "BLEU Score: 0.0000\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "Total examples: 3\n",
      "Total time: 1.539s\n",
      "Average time per translation: 0.513s\n",
      "Average BLEU score: 0.0000\n",
      "Max BLEU score: 0.0000\n",
      "Min BLEU score: 0.0000\n",
      "\n",
      "================================================================================\n",
      "TESTING WITH BEAM SEARCH\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "MACHINE TRANSLATION TESTING RESULTS\n",
      "================================================================================\n",
      "Model: Transformer (Attention is All You Need)\n",
      "Decoding: Beam Search\n",
      "Beam size: 3\n",
      "Temperature: 0.8\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test 1/1\n",
      "----------------------------------------\n",
      "English:  Nice to meet you\n",
      "Expected: 很高兴见到你\n",
      "Generated: [Empty translation]\n",
      "Time: 0.066s\n",
      "BLEU Score: 0.0000\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "Total examples: 1\n",
      "Total time: 0.066s\n",
      "Average time per translation: 0.066s\n",
      "Average BLEU score: 0.0000\n",
      "Max BLEU score: 0.0000\n",
      "Min BLEU score: 0.0000\n",
      "\n",
      "================================================================================\n",
      "QUICK PERFORMANCE TEST\n",
      "================================================================================\n",
      "Testing translation speed:\n",
      "----------------------------------------\n",
      "Test 1: 0.008s\n",
      "  Input:  Hello\n",
      "  Output: [Empty translation]\n",
      "\n",
      "Test 2: 0.007s\n",
      "  Input:  I am happy\n",
      "  Output: [Empty translation]\n",
      "\n",
      "Test 3: 0.228s\n",
      "  Input:  The weather is nice today\n",
      "  Output: [Empty translation]\n",
      "\n",
      "================================================================================\n",
      "TRANSFORMER TESTING COMPLETE\n",
      "================================================================================\n",
      "You can now use:\n",
      "- transformer_tester.translate_single('your text here')\n",
      "- transformer_tester.interactive_translation()\n",
      "- transformer_tester.test_examples(your_test_cases)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fixed Comprehensive Machine Translation Testing Code for Transformer Model\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "class TransformerTranslationTester:\n",
    "    \"\"\"\n",
    "    Comprehensive tester for the Transformer machine translation model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device, max_length=100):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load tokenizers\n",
    "        self.tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "        \n",
    "        # Special tokens\n",
    "        self.bos_token = 101  # [CLS] token used as BOS\n",
    "        self.eos_token = 102  # [SEP] token used as EOS\n",
    "        self.pad_token = 0    # [PAD] token\n",
    "        \n",
    "        print(\"=== Transformer Translation Tester Initialized ===\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"English tokenizer vocab size: {self.tokenizer_en.vocab_size}\")\n",
    "        print(f\"Chinese tokenizer vocab size: {self.tokenizer_zh.vocab_size}\")\n",
    "        print(f\"Max sequence length: {max_length}\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess input text\"\"\"\n",
    "        # Basic cleaning\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single space\n",
    "        return text\n",
    "\n",
    "    def encode_english(self, text):\n",
    "        \"\"\"Encode English text to token IDs\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        tokens = self.tokenizer_en(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens['input_ids'].to(self.device)\n",
    "\n",
    "    def decode_chinese(self, token_ids):\n",
    "        \"\"\"Decode Chinese token IDs to text\"\"\"\n",
    "        # Remove padding tokens and special tokens\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.cpu().tolist()\n",
    "        \n",
    "        # Remove padding (0), BOS (101), and EOS (102) tokens\n",
    "        cleaned_tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in [0, 101, 102]:\n",
    "                cleaned_tokens.append(token_id)\n",
    "            elif token_id == 102:  # Stop at EOS token\n",
    "                break\n",
    "        \n",
    "        if not cleaned_tokens:\n",
    "            return \"[Empty translation]\"\n",
    "        \n",
    "        try:\n",
    "            text = self.tokenizer_zh.decode(cleaned_tokens, skip_special_tokens=True)\n",
    "            # Clean up extra spaces for Chinese\n",
    "            text = re.sub(r'\\s+', '', text)  # Remove all spaces for Chinese\n",
    "            return text if text else \"[Empty translation]\"\n",
    "        except Exception as e:\n",
    "            return f\"[Decoding error: {str(e)}]\"\n",
    "\n",
    "    def translate_single(self, english_text, use_beam_search=False, beam_size=3, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Translate a single English sentence to Chinese\n",
    "        \n",
    "        Args:\n",
    "            english_text: Input English text\n",
    "            use_beam_search: Whether to use beam search (simplified implementation)\n",
    "            beam_size: Beam search size (only used if use_beam_search=True)\n",
    "            temperature: Sampling temperature for generation\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode input\n",
    "            source_ids = self.encode_english(english_text)\n",
    "            \n",
    "            if not use_beam_search:\n",
    "                # Use the model's built-in greedy generation\n",
    "                generated = self.model.generate(\n",
    "                    source_ids, \n",
    "                    max_length=self.max_length,\n",
    "                    bos_token=self.bos_token,\n",
    "                    eos_token=self.eos_token,\n",
    "                    pad_token=self.pad_token\n",
    "                )\n",
    "                translation = self.decode_chinese(generated[0])\n",
    "            else:\n",
    "                # Use simplified beam search\n",
    "                translation = self._beam_search_translate(source_ids, beam_size, temperature)\n",
    "        \n",
    "        return translation\n",
    "\n",
    "    def _beam_search_translate(self, source_ids, beam_size=3, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Simplified beam search implementation for Transformer model\n",
    "        \"\"\"\n",
    "        batch_size = source_ids.size(0)\n",
    "        device = source_ids.device\n",
    "        \n",
    "        # Encode source\n",
    "        src_mask = self.model.create_padding_mask(source_ids)\n",
    "        encoder_output = self.model.encode(source_ids, src_mask)\n",
    "        \n",
    "        # Initialize beams: (sequence, score)\n",
    "        beams = [(torch.tensor([[self.bos_token]], device=device), 0.0)]\n",
    "        \n",
    "        for step in range(self.max_length):\n",
    "            new_beams = []\n",
    "            \n",
    "            for seq, score in beams:\n",
    "                # If sequence already ended, keep it\n",
    "                if seq[0, -1].item() == self.eos_token:\n",
    "                    new_beams.append((seq, score))\n",
    "                    continue\n",
    "                \n",
    "                # Create masks for current sequence\n",
    "                tgt_mask = self.model.create_padding_mask(seq) & \\\n",
    "                          self.model.create_look_ahead_mask(seq.size(1)).to(device)\n",
    "                \n",
    "                # Get decoder output\n",
    "                decoder_output = self.model.decode(seq, encoder_output, tgt_mask, src_mask)\n",
    "                \n",
    "                # Get next token probabilities\n",
    "                logits = self.model.output_projection(decoder_output[:, -1, :])\n",
    "                \n",
    "                # Apply temperature\n",
    "                if temperature != 1.0:\n",
    "                    logits = logits / temperature\n",
    "                \n",
    "                # Get probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Get top k candidates\n",
    "                top_probs, top_indices = torch.topk(probs, min(beam_size, probs.size(-1)))\n",
    "                \n",
    "                for i in range(top_probs.size(1)):\n",
    "                    token_id = top_indices[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                    token_prob = top_probs[0, i].item()\n",
    "                    new_seq = torch.cat([seq, token_id], dim=1)\n",
    "                    new_score = score + math.log(token_prob + 1e-10)\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "            \n",
    "            # Keep only top beams\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "            \n",
    "            # Check if all beams ended\n",
    "            if all(seq[0, -1].item() == self.eos_token for seq, _ in beams):\n",
    "                break\n",
    "        \n",
    "        # Return best beam\n",
    "        best_seq = beams[0][0]\n",
    "        return self.decode_chinese(best_seq[0])\n",
    "\n",
    "    def calculate_bleu_score(self, reference, candidate):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score (simplified version)\n",
    "        \"\"\"\n",
    "        def get_ngrams(tokens, n):\n",
    "            return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "        \n",
    "        # Tokenize (character-level for Chinese)\n",
    "        ref_tokens = list(reference.replace(' ', ''))\n",
    "        cand_tokens = list(candidate.replace(' ', ''))\n",
    "        \n",
    "        if len(cand_tokens) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate precision for n-grams (1 to 4)\n",
    "        precisions = []\n",
    "        for n in range(1, 5):\n",
    "            ref_ngrams = Counter(get_ngrams(ref_tokens, n))\n",
    "            cand_ngrams = Counter(get_ngrams(cand_tokens, n))\n",
    "            \n",
    "            if len(cand_ngrams) == 0:\n",
    "                precisions.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            matches = sum((ref_ngrams & cand_ngrams).values())\n",
    "            precision = matches / len(get_ngrams(cand_tokens, n))\n",
    "            precisions.append(precision)\n",
    "        \n",
    "        # Calculate brevity penalty\n",
    "        ref_len = len(ref_tokens)\n",
    "        cand_len = len(cand_tokens)\n",
    "        \n",
    "        if cand_len > ref_len:\n",
    "            bp = 1.0\n",
    "        else:\n",
    "            bp = math.exp(1 - ref_len / (cand_len + 1e-10))\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        if min(precisions) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        bleu = bp * math.exp(sum(math.log(p + 1e-10) for p in precisions) / 4)\n",
    "        return bleu\n",
    "\n",
    "    def test_examples(self, test_cases, use_beam_search=False, beam_size=3, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Test the model on multiple examples\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MACHINE TRANSLATION TESTING RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Model: Transformer (Attention is All You Need)\")\n",
    "        print(f\"Decoding: {'Beam Search' if use_beam_search else 'Greedy'}\")\n",
    "        if use_beam_search:\n",
    "            print(f\"Beam size: {beam_size}\")\n",
    "            print(f\"Temperature: {temperature}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        results = []\n",
    "        total_time = 0\n",
    "        \n",
    "        for i, (english, expected_chinese) in enumerate(test_cases, 1):\n",
    "            print(f\"\\nTest {i}/{len(test_cases)}\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"English:  {english}\")\n",
    "            \n",
    "            # Translate\n",
    "            start_time = time.time()\n",
    "            translation = self.translate_single(english, use_beam_search, beam_size, temperature)\n",
    "            translation_time = time.time() - start_time\n",
    "            total_time += translation_time\n",
    "            \n",
    "            print(f\"Expected: {expected_chinese}\")\n",
    "            print(f\"Generated: {translation}\")\n",
    "            print(f\"Time: {translation_time:.3f}s\")\n",
    "            \n",
    "            # Calculate BLEU score if reference is provided\n",
    "            if expected_chinese and expected_chinese != \"\":\n",
    "                bleu_score = self.calculate_bleu_score(expected_chinese, translation)\n",
    "                print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "            else:\n",
    "                bleu_score = None\n",
    "                print(\"BLEU Score: N/A (no reference)\")\n",
    "            \n",
    "            results.append({\n",
    "                'english': english,\n",
    "                'expected': expected_chinese,\n",
    "                'translation': translation,\n",
    "                'time': translation_time,\n",
    "                'bleu': bleu_score\n",
    "            })\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total examples: {len(test_cases)}\")\n",
    "        print(f\"Total time: {total_time:.3f}s\")\n",
    "        print(f\"Average time per translation: {total_time/len(test_cases):.3f}s\")\n",
    "        \n",
    "        bleu_scores = [r['bleu'] for r in results if r['bleu'] is not None]\n",
    "        if bleu_scores:\n",
    "            print(f\"Average BLEU score: {np.mean(bleu_scores):.4f}\")\n",
    "            print(f\"Max BLEU score: {max(bleu_scores):.4f}\")\n",
    "            print(f\"Min BLEU score: {min(bleu_scores):.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def interactive_translation(self):\n",
    "        \"\"\"Interactive translation mode\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INTERACTIVE TRANSLATION MODE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Enter English text to translate (type 'quit' to exit)\")\n",
    "        print(\"Type 'beam' to toggle beam search mode\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        use_beam_search = False\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(f\"\\nEnglish ({'Beam' if use_beam_search else 'Greedy'}): \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if user_input.lower() == 'beam':\n",
    "                    use_beam_search = not use_beam_search\n",
    "                    print(f\"Switched to {'Beam Search' if use_beam_search else 'Greedy'} mode\")\n",
    "                    continue\n",
    "                \n",
    "                if not user_input:\n",
    "                    print(\"Please enter some text.\")\n",
    "                    continue\n",
    "                \n",
    "                # Translate\n",
    "                start_time = time.time()\n",
    "                translation = self.translate_single(user_input, use_beam_search=use_beam_search)\n",
    "                translation_time = time.time() - start_time\n",
    "                \n",
    "                print(f\"Chinese:  {translation}\")\n",
    "                print(f\"Time: {translation_time:.3f}s\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nGoodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Test cases for English to Chinese translation\n",
    "TRANSFORMER_TEST_CASES = [\n",
    "    # Basic greetings and common phrases\n",
    "    (\"Hello\", \"你好\"),\n",
    "    (\"Good morning\", \"早上好\"),\n",
    "    (\"Thank you\", \"谢谢\"),\n",
    "    (\"Nice to meet you\", \"很高兴见到你\"),\n",
    "    (\"Goodbye\", \"再见\"),\n",
    "    \n",
    "    # Simple sentences\n",
    "    (\"I love you\", \"我爱你\"),\n",
    "    (\"What is your name?\", \"你叫什么名字？\"),\n",
    "    (\"Where are you from?\", \"你来自哪里？\"),\n",
    "    (\"How are you?\", \"你好吗？\"),\n",
    "    (\"Can you help me?\", \"你能帮助我吗？\"),\n",
    "    \n",
    "    # Daily life\n",
    "    (\"I am hungry\", \"我饿了\"),\n",
    "    (\"The weather is nice\", \"天气很好\"),\n",
    "    (\"I need to go to work\", \"我需要去工作\"),\n",
    "    (\"Let's have dinner\", \"我们吃晚饭吧\"),\n",
    "    (\"I am learning Chinese\", \"我在学中文\"),\n",
    "    \n",
    "    # Without reference translations (for testing only)\n",
    "    (\"Machine learning is important\", \"\"),\n",
    "    (\"I like to read books\", \"\"),\n",
    "    (\"The cat is sleeping\", \"\"),\n",
    "    (\"Tomorrow is Monday\", \"\"),\n",
    "    (\"Coffee tastes good\", \"\"),\n",
    "]\n",
    "\n",
    "# Initialize the corrected tester\n",
    "print(\"Initializing Transformer translation tester...\")\n",
    "transformer_tester = TransformerTranslationTester(model, device, max_length=100)\n",
    "\n",
    "# Check if model was trained\n",
    "has_trained_model = 'history' in globals() and history is not None\n",
    "if not has_trained_model:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"WARNING: Model appears to be untrained!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"The model has not been trained yet, so translations will be random.\")\n",
    "    print(\"To get meaningful results, you should train the model first.\")\n",
    "    print(\"You can still test the translation pipeline, but expect poor quality.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Test with simple examples first (Greedy decoding)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING SIMPLE EXAMPLES (Greedy Decoding)\")\n",
    "print(\"=\"*80)\n",
    "simple_results = transformer_tester.test_examples(TRANSFORMER_TEST_CASES[:3], use_beam_search=False)\n",
    "\n",
    "# Test one example with beam search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING WITH BEAM SEARCH\")\n",
    "print(\"=\"*80)\n",
    "beam_results = transformer_tester.test_examples(TRANSFORMER_TEST_CASES[3:4], use_beam_search=True, beam_size=3, temperature=0.8)\n",
    "\n",
    "# Quick performance test\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUICK PERFORMANCE TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_sentences = [\n",
    "    \"Hello\",\n",
    "    \"I am happy\",\n",
    "    \"The weather is nice today\"\n",
    "]\n",
    "\n",
    "print(\"Testing translation speed:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    start_time = time.time()\n",
    "    translation = transformer_tester.translate_single(sentence)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Test {i}: {end_time - start_time:.3f}s\")\n",
    "    print(f\"  Input:  {sentence}\")\n",
    "    print(f\"  Output: {translation}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRANSFORMER TESTING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"You can now use:\")\n",
    "print(\"- transformer_tester.translate_single('your text here')\")\n",
    "print(\"- transformer_tester.interactive_translation()\")\n",
    "print(\"- transformer_tester.test_examples(your_test_cases)\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
