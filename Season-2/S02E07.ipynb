{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding: Machine Translation by RNN\n",
    "\n",
    " - Dataset: wmt-17, en-zh, select 5m high-quality pairs of data\n",
    " - Model: Seq2seq with Encoder & Decoder framework\n",
    " - GPU: 4090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset Size:  25134743\n",
      "Filtered Dataset Size:  1141860\n",
      "Dataset Size:  1141860\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Zambia (7)\n",
      "赞比亚(7)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "15:00 to 18:00 Informal consultations (closed) Conference Room 5 (NLB)\n",
      "下午3:00－6:00 非正式磋商(闭门会议) 第5会议室(北草坪会议大楼)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spain\n",
      "西班牙\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mr. Robert Morrison\n",
      "Robert Morrison先生 加拿大自然资源部\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This satisfied the kids, but not the husband.\n",
      "\"孩子们得到了满意的答案, 但她的丈夫却没有。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shutaro Omura (Japan)\n",
      "Shutaro Omura（日本）\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\"Oh,\" she says, \"what am I THINKING about!\n",
      "——”    “哦，”她说，“你看我在想些什么啊！\n",
      "----------------------------------------------------------------------------------------------------\n",
      "30 June 2005\n",
      "2005年6月30日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "11 July-5 August 2005\n",
      "2005年7月11日-8月5日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "May 2011\n",
      "2011年5月\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download the data & select 5m high-quality pairs\n",
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# load full wmt-17 en-zh dataset\n",
    "full_dataset = load_dataset(\"wmt/wmt17\", \"zh-en\", split=\"train\")\n",
    "\n",
    "# Length & Ratio filter\n",
    "def is_high_quality(x):\n",
    "    en = x[\"translation\"][\"en\"]\n",
    "    zh = x[\"translation\"][\"zh\"]\n",
    "    if not en or not zh:\n",
    "        return False\n",
    "    if len(en) < 3 or len(zh) < 3:\n",
    "        return False\n",
    "    if len(en) > 100 or len(zh) > 100:\n",
    "        return False\n",
    "    ratio = len(en) / len(zh)\n",
    "    if ratio < 0.5 or ratio > 2:\n",
    "        return False\n",
    "    if not re.search(r'[\\u4e00-\\u9fff]', zh):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_dataset = full_dataset.filter(is_high_quality, num_proc=10)\n",
    "\n",
    "dataset = filtered_dataset.select(range(min(5_000_000, len(filtered_dataset))))\n",
    "\n",
    "print(\"Full Dataset Size: \", len(full_dataset))\n",
    "print(\"Filtered Dataset Size: \", len(filtered_dataset))\n",
    "print(\"Dataset Size: \", len(dataset))\n",
    "\n",
    "# print 10 samples\n",
    "sample = dataset.shuffle(seed=42).select(range(10))\n",
    "print(\"-\"*100)\n",
    "for i in sample:\n",
    "    print(i[\"translation\"][\"en\"])\n",
    "    print(i[\"translation\"][\"zh\"])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1084767\n",
      "Validation dataset size: 57093\n",
      "Train DataLoader: 33899 batches\n",
      "Validation DataLoader: 1785 batches\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 32\n",
      "Source example:  Thus, business fixed investment is expected to rise in 1996, albeit slowly.\n",
      "Source tokens: tensor([  101, 33115,   117, 14155, 37770, 37933, 10124, 25973, 10114, 28710,\n",
      "        10106, 10389,   117, 98892, 22235, 63088,   119,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Target example:  因此 , 1996年 企业 固定 投资 可望 增加 , 尽管 幅度 不 高 。\n",
      "Target tokens: tensor([  101,  3000,  4792,   117, 10389,  3642,  2215,  2090,  3013,  3388,\n",
      "         4037,  7507,  2756,  4468,  3148,  2598,   117,  3480,  6098,  3630,\n",
      "         3670,  2080,  8595,  1882,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 32\n",
      "Source example: 74 and A/60/640/Add.1, para. 6\n",
      "Source tokens: tensor([  101, 12535, 10111,   138,   120, 10709,   120, 30127,   120, 25474,\n",
      "        10162,   119,   122,   117, 10220,   119,   127,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Target example: (A/60/640/Add.1，第6段)\n",
      "Target tokens: tensor([  101,   113,   138,   120, 10709,   120, 30127,   120, 25474, 10162,\n",
      "          119,   122, 10064,  6063,   127,  4823,   114,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Dataset and DataLoader for training\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=100):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset wrapper for HuggingFace translation dataset\n",
    "        \n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset with translation pairs\n",
    "            tokenizer_en: English tokenizer (optional, can be added later)\n",
    "            tokenizer_zh: Chinese tokenizer (optional, can be added later)\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        en_text = item[\"translation\"][\"en\"]\n",
    "        zh_text = item[\"translation\"][\"zh\"]\n",
    "\n",
    "        en_tokens = self.tokenizer(en_text, \n",
    "                                        max_length=self.max_length, \n",
    "                                        padding='max_length', \n",
    "                                        truncation=True, \n",
    "                                        return_tensors='pt')\n",
    "            \n",
    "            # Tokenize Chinese (target)\n",
    "        zh_tokens = self.tokenizer(zh_text, \n",
    "                                        max_length=self.max_length, \n",
    "                                        padding='max_length', \n",
    "                                        truncation=True, \n",
    "                                        return_tensors='pt')\n",
    "            \n",
    "        return {\n",
    "                'source_ids': en_tokens['input_ids'].squeeze(),\n",
    "                'target_ids': zh_tokens['input_ids'].squeeze(),\n",
    "                'source_text': en_text,\n",
    "                'target_text': zh_text\n",
    "        }\n",
    "\n",
    "def create_dataloaders(dataset, batch_size=32, num_workers=4, train_split=0.95):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders from HuggingFace dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with translation pairs\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        train_split: Fraction of data to use for training\n",
    "    \n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader, train_dataset, val_dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split dataset into train and validation\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    \n",
    "    # Create indices for splitting\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices, \n",
    "                                                train_size=train_size, \n",
    "                                                random_state=42)\n",
    "    \n",
    "    # Create train and validation datasets\n",
    "    train_dataset_hf = dataset.select(train_indices)\n",
    "    val_dataset_hf = dataset.select(val_indices)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    \n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = TranslationDataset(train_dataset_hf, tokenizer)\n",
    "    val_dataset = TranslationDataset(val_dataset_hf, tokenizer)\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Train DataLoader: {len(train_dataloader)} batches\")\n",
    "    print(f\"Validation DataLoader: {len(val_dataloader)} batches\")\n",
    "    \n",
    "    return train_dataloader, val_dataloader, train_dataset, val_dataset\n",
    "\n",
    "def test_dataloader(dataloader):\n",
    "    \"\"\"Test the DataLoader by printing a sample batch\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Sample batch from DataLoader:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        print(f\"Batch size: {len(batch['source_text'])}\")\n",
    "        print(f\"Source example: {batch['source_text'][0]}\")\n",
    "        print(f\"Source tokens: {batch['source_ids'][0]}\")\n",
    "        print(f\"Target example: {batch['target_text'][0]}\")\n",
    "        print(f\"Target tokens: {batch['target_ids'][0]}\")\n",
    "        break\n",
    "\n",
    "train_dataloader, val_dataloader, _, _ = create_dataloaders(dataset)\n",
    "test_dataloader(train_dataloader)\n",
    "test_dataloader(val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
