{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding: Machine Translation by RNN\n",
    "\n",
    " - Dataset: wmt-17, en-zh, select 5m high-quality pairs of data\n",
    " - Model: Seq2seq with Encoder & Decoder framework\n",
    " - GPU: 4090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Encoder-Decoder Architecture\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Sequence-to-Sequence (Seq2Seq) model with Encoder-Decoder architecture is a neural network framework designed for tasks where both input and output are sequences of variable length, such as machine translation (English → Chinese in your case).\n",
    "\n",
    "```\n",
    "Input Sequence (English):  \"Hello world\"\n",
    "                              ↓\n",
    "                          [ENCODER]\n",
    "                              ↓\n",
    "                        Context Vector\n",
    "                              ↓\n",
    "                          [DECODER]\n",
    "                              ↓\n",
    "Output Sequence (Chinese): \"你好世界\"\n",
    "```\n",
    "\n",
    "## Architecture Components\n",
    "\n",
    "### 1. Encoder\n",
    "\n",
    "The encoder processes the input sequence and compresses the information into a fixed-size context vector (also called thought vector).\n",
    "\n",
    "```\n",
    "Input: [w1, w2, w3, ..., wn]\n",
    "       ↓    ↓    ↓       ↓\n",
    "    [RNN][RNN][RNN]...[RNN]\n",
    "       ↓    ↓    ↓       ↓\n",
    "    [h1] [h2] [h3] ... [hn] → Context Vector (hn)\n",
    "```\n",
    "\n",
    "#### Key Components:\n",
    "- **Embedding Layer**: Converts input tokens to dense vectors\n",
    "- **RNN Layers**: LSTM/GRU cells process the sequence sequentially\n",
    "- **Hidden States**: Capture information at each time step\n",
    "- **Final Context**: Last hidden state becomes the context vector\n",
    "\n",
    "```python\n",
    "# Pseudo-code structure\n",
    "class Encoder:\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers)\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden  # Context vectors\n",
    "```\n",
    "\n",
    "### 2. Decoder\n",
    "\n",
    "The decoder generates the output sequence one token at a time, using the context vector from the encoder.\n",
    "\n",
    "```\n",
    "Context Vector (C) → [RNN] → [RNN] → [RNN] → ... → [RNN]\n",
    "                      ↓       ↓       ↓             ↓\n",
    "                    [y1]    [y2]    [y3]   ...   [yn]\n",
    "```\n",
    "\n",
    "#### Key Components:\n",
    "- **Initial State**: Initialized with encoder's context vector\n",
    "- **RNN Layers**: Generate hidden states for each output position\n",
    "- **Output Projection**: Maps hidden states to vocabulary probabilities\n",
    "- **Softmax**: Converts logits to probability distribution\n",
    "\n",
    "```python\n",
    "# Pseudo-code structure\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers)\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, target_seq, encoder_hidden, encoder_cell):\n",
    "        embedded = self.embedding(target_seq)\n",
    "        outputs, _ = self.rnn(embedded, encoder_hidden)\n",
    "        predictions = self.output_projection(outputs)\n",
    "        return predictions\n",
    "```\n",
    "\n",
    "## Complete Architecture Flow\n",
    "\n",
    "### Training Phase\n",
    "\n",
    "```\n",
    "1. Input Processing:\n",
    "   English: \"Hello world\" → [101, 7592, 2088, 102] (tokenized)\n",
    "   Chinese: \"[BOS] 你好世界 [EOS]\" → [101, 872, 1962, 686, 102] (tokenized)\n",
    "\n",
    "2. Encoder Forward Pass:\n",
    "   Input: [101, 7592, 2088, 102]\n",
    "   ↓\n",
    "   Embedding: [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]\n",
    "   ↓\n",
    "   LSTM/GRU: h1, h2, h3, h4 → Context Vector (h4)\n",
    "\n",
    "3. Decoder Forward Pass:\n",
    "   Initial State: (h4) from encoder\n",
    "   Input: [101, 872, 1962, 686]\n",
    "   ↓\n",
    "   LSTM/GRU: generates hidden states for each position\n",
    "   ↓\n",
    "   Output Projection: [vocab_size] logits for each position\n",
    "   ↓\n",
    "   Loss Calculation: CrossEntropy with targets [872, 1962, 686, 102]\n",
    "```\n",
    "\n",
    "### Inference Phase\n",
    "\n",
    "```\n",
    "1. Encode input sequence: \"Hello world\"\n",
    "2. Initialize decoder with encoder's context vector\n",
    "3. Start with [BOS] token\n",
    "4. Generate tokens one by one:\n",
    "   - Input: [BOS] → Output: 你 (probability distribution)\n",
    "   - Input: [BOS] 你 → Output: 好\n",
    "   - Input: [BOS] 你 好 → Output: 世\n",
    "   - Input: [BOS] 你 好 世 → Output: 界\n",
    "   - Input: [BOS] 你 好 世 界 → Output: [EOS] (stop)\n",
    "```\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Encoder\n",
    "```\n",
    "h_t = LSTM/GRU(embedding(x_t), h_{t-1})\n",
    "context = h_n  # Final hidden state\n",
    "```\n",
    "\n",
    "### Decoder\n",
    "```\n",
    "s_t = LSTM/GRU(embedding(y_{t-1}), s_{t-1})  # s_0 = context\n",
    "P(y_t | y_1...y_{t-1}, x) = softmax(W_s * s_t + b_s)\n",
    "```\n",
    "\n",
    "### Loss Function\n",
    "```\n",
    "Loss = -∑∑ log P(y_t^i | y_1^i...y_{t-1}^i, x^i)\n",
    "```\n",
    "\n",
    "## Architecture Advantages\n",
    "\n",
    "1. **Variable Length Handling**: Can process sequences of different lengths\n",
    "2. **End-to-End Learning**: Jointly optimizes encoder and decoder\n",
    "3. **Context Preservation**: Encoder captures semantic meaning in context vector\n",
    "4. **Language Agnostic**: Works for any language pair\n",
    "\n",
    "## Architecture Limitations\n",
    "\n",
    "1. **Information Bottleneck**: Fixed-size context vector may lose information\n",
    "2. **Long Sequence Problem**: Difficulty with very long input sequences\n",
    "3. **Sequential Processing**: Cannot parallelize during inference\n",
    "\n",
    "## Improvements & Variants\n",
    "\n",
    "1. **Attention Mechanism**: Allows decoder to focus on relevant encoder states\n",
    "2. **Bidirectional Encoder**: Processes sequence in both directions\n",
    "3. **Beam Search**: Better decoding strategy than greedy search\n",
    "4. **Teacher Forcing**: Training technique using ground truth as decoder input\n",
    "\n",
    "## Implementation Architecture for Your Project\n",
    "\n",
    "Based on your dataset (WMT-17 EN-ZH) and tokenizers (BERT-based), here's the recommended architecture:\n",
    "\n",
    "```\n",
    "Input: English sentence (max_length=100)\n",
    "↓\n",
    "BERT Tokenizer (vocab_size=30522) → Token IDs\n",
    "↓\n",
    "Embedding Layer (30522 → 512)\n",
    "↓\n",
    "Encoder LSTM/GRU (512 → 1024, num_layers=2)\n",
    "↓\n",
    "Context Vector (1024-dim)\n",
    "↓\n",
    "Decoder LSTM/GRU (512 → 1024, num_layers=2)\n",
    "↓\n",
    "Output Projection (1024 → 21128)\n",
    "↓\n",
    "Chinese Token IDs → BERT Tokenizer → Chinese sentence\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/archer/archer/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset Size:  25134743\n",
      "Filtered Dataset Size:  1141860\n",
      "Dataset Size:  1141860\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Zambia (7)\n",
      "赞比亚(7)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "15:00 to 18:00 Informal consultations (closed) Conference Room 5 (NLB)\n",
      "下午3:00－6:00 非正式磋商(闭门会议) 第5会议室(北草坪会议大楼)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spain\n",
      "西班牙\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mr. Robert Morrison\n",
      "Robert Morrison先生 加拿大自然资源部\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This satisfied the kids, but not the husband.\n",
      "\"孩子们得到了满意的答案, 但她的丈夫却没有。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shutaro Omura (Japan)\n",
      "Shutaro Omura（日本）\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\"Oh,\" she says, \"what am I THINKING about!\n",
      "——”    “哦，”她说，“你看我在想些什么啊！\n",
      "----------------------------------------------------------------------------------------------------\n",
      "30 June 2005\n",
      "2005年6月30日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "11 July-5 August 2005\n",
      "2005年7月11日-8月5日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "May 2011\n",
      "2011年5月\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download the data & select 5m high-quality pairs\n",
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# load full wmt-17 en-zh dataset\n",
    "full_dataset = load_dataset(\"wmt/wmt17\", \"zh-en\", split=\"train\")\n",
    "\n",
    "# Length & Ratio filter\n",
    "def is_high_quality(x):\n",
    "    en = x[\"translation\"][\"en\"]\n",
    "    zh = x[\"translation\"][\"zh\"]\n",
    "    if not en or not zh:\n",
    "        return False\n",
    "    if len(en) < 3 or len(zh) < 3:\n",
    "        return False\n",
    "    if len(en) > 100 or len(zh) > 100:\n",
    "        return False\n",
    "    ratio = len(en) / len(zh)\n",
    "    if ratio < 0.5 or ratio > 2:\n",
    "        return False\n",
    "    if not re.search(r'[\\u4e00-\\u9fff]', zh):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_dataset = full_dataset.filter(is_high_quality, num_proc=10)\n",
    "\n",
    "dataset = filtered_dataset.select(range(min(5_000_000, len(filtered_dataset))))\n",
    "\n",
    "print(\"Full Dataset Size: \", len(full_dataset))\n",
    "print(\"Filtered Dataset Size: \", len(filtered_dataset))\n",
    "print(\"Dataset Size: \", len(dataset))\n",
    "\n",
    "# print 10 samples\n",
    "sample = dataset.shuffle(seed=42).select(range(10))\n",
    "print(\"-\"*100)\n",
    "for i in sample:\n",
    "    print(i[\"translation\"][\"en\"])\n",
    "    print(i[\"translation\"][\"zh\"])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for en: 30522\n",
      "Vocab size for zh: 21128\n",
      "Train dataset size: 1084767\n",
      "Validation dataset size: 57093\n",
      "Train DataLoader: 33899 batches\n",
      "Validation DataLoader: 1785 batches\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 32\n",
      "Source example: There were 132 cases of beatings.\n",
      "Source tokens: tensor([  101,  2045,  2020, 14078,  3572,  1997,  6012,  2015,  1012,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Target example: 发生 了 132 起 殴打 事件 。\n",
      "Target tokens: tensor([ 101, 1355, 4495,  749, 9306, 6629, 3666, 2802,  752,  816,  511,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 32\n",
      "Source example: 74 and A/60/640/Add.1, para. 6\n",
      "Source tokens: tensor([  101,  6356,  1998,  1037,  1013,  3438,  1013, 19714,  1013,  5587,\n",
      "         1012,  1015,  1010, 11498,  1012,  1020,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Target example: (A/60/640/Add.1，第6段)\n",
      "Target tokens: tensor([  101,   113,   100,   120,  8183,   120, 11410,   120,   100,   119,\n",
      "          122,  8024,  5018,   127,  3667,   114,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Dataset and DataLoader for training\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer_en, tokenizer_zh, max_length=100):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset wrapper for HuggingFace translation dataset\n",
    "        \n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset with translation pairs\n",
    "            tokenizer_en: English tokenizer (optional, can be added later)\n",
    "            tokenizer_zh: Chinese tokenizer (optional, can be added later)\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_zh = tokenizer_zh\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        en_text = item[\"translation\"][\"en\"]\n",
    "        zh_text = item[\"translation\"][\"zh\"]\n",
    "\n",
    "        en_tokens = self.tokenizer_en(en_text, \n",
    "                                        max_length=self.max_length, \n",
    "                                        padding='max_length', \n",
    "                                        truncation=True, \n",
    "                                        return_tensors='pt')\n",
    "            \n",
    "        zh_tokens = self.tokenizer_zh(zh_text, \n",
    "                                        max_length=self.max_length, \n",
    "                                        padding='max_length', \n",
    "                                        truncation=True, \n",
    "                                        return_tensors='pt')\n",
    "            \n",
    "        return {\n",
    "                'source_ids': en_tokens['input_ids'].squeeze(),\n",
    "                'target_ids': zh_tokens['input_ids'].squeeze(),\n",
    "                'source_text': en_text,\n",
    "                'target_text': zh_text\n",
    "        }\n",
    "\n",
    "def create_dataloaders(dataset, batch_size=32, num_workers=4, train_split=0.95):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders from HuggingFace dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with translation pairs\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        train_split: Fraction of data to use for training\n",
    "    \n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader, train_dataset, val_dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split dataset into train and validation\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    \n",
    "    # Create indices for splitting\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices, \n",
    "                                                train_size=train_size, \n",
    "                                                random_state=42)\n",
    "    \n",
    "    # Create train and validation datasets\n",
    "    train_dataset_hf = dataset.select(train_indices)\n",
    "    val_dataset_hf = dataset.select(val_indices)\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "    # get vocab sizes\n",
    "    vocab_size_en = tokenizer_en.vocab_size\n",
    "    vocab_size_zh = tokenizer_zh.vocab_size\n",
    "\n",
    "    print(f\"Vocab size for en: {vocab_size_en}\")\n",
    "    print(f\"Vocab size for zh: {vocab_size_zh}\")\n",
    "\n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = TranslationDataset(train_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "    val_dataset = TranslationDataset(val_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Train DataLoader: {len(train_dataloader)} batches\")\n",
    "    print(f\"Validation DataLoader: {len(val_dataloader)} batches\")\n",
    "    \n",
    "    return train_dataloader, val_dataloader, vocab_size_en, vocab_size_zh\n",
    "\n",
    "def test_dataloader(dataloader):\n",
    "    \"\"\"Test the DataLoader by printing a sample batch\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Sample batch from DataLoader:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        print(f\"Batch size: {len(batch['source_text'])}\")\n",
    "        print(f\"Source example: {batch['source_text'][0]}\")\n",
    "        print(f\"Source tokens: {batch['source_ids'][0]}\")\n",
    "        print(f\"Target example: {batch['target_text'][0]}\")\n",
    "        print(f\"Target tokens: {batch['target_ids'][0]}\")\n",
    "        break\n",
    "\n",
    "train_dataloader, val_dataloader, encoder_vocab_size, decoder_vocab_size = create_dataloaders(dataset)\n",
    "test_dataloader(train_dataloader)\n",
    "test_dataloader(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Seq2Seq Model with GRU Architecture ===\n",
      "Device: cpu\n",
      "Encoder Vocabulary Size: 30,522\n",
      "Decoder Vocabulary Size: 21,128\n",
      "Embedding Size: 256\n",
      "Hidden Size: 512\n",
      "Number of Layers: 2\n",
      "Dropout Rate: 0.1\n",
      "RNN Type: GRU (Gated Recurrent Unit)\n",
      "\n",
      "Total Parameters: 29,578,376\n",
      "Trainable Parameters: 29,578,376\n",
      "Model Size: 112.83 MB\n",
      "\n",
      "=== Model Architecture ===\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(30522, 256, padding_idx=0)\n",
      "    (rnn): GRU(256, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(21128, 256, padding_idx=0)\n",
      "    (rnn): GRU(256, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (output_projection): Linear(in_features=512, out_features=21128, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "=== Testing Model Forward Pass ===\n",
      "Source shape: torch.Size([32, 100])\n",
      "Target shape: torch.Size([32, 100])\n",
      "Output shape: torch.Size([32, 100, 21128])\n",
      "Expected shape: [batch_size, target_len, decoder_vocab_size]\n",
      "Actual shape: [32, 100, 21128]\n",
      "\n",
      "=== Testing Model Generation ===\n",
      "Generated sequence shape: torch.Size([2, 100])\n",
      "Generated tokens (first sample): [13486, 6499, 6499, 11491, 16203, 16203, 12553, 12553, 2852, 12365, 20662, 12365, 2160, 9771, 20910, 7604, 3739, 2429, 3186, 6462, 6462, 12396, 12396, 2429, 2429, 16977, 11276, 11276, 10845, 18564, 15309, 15309, 14542, 1832, 5484, 19561, 17023, 17023, 2380, 13054, 13054, 6048, 3832, 9414, 17122, 3887, 12191, 84, 20482, 2099, 20482, 17381, 13770, 720, 6889, 20023, 8837, 12948, 1383, 12948, 400, 400, 17111, 8097, 551, 17231, 17231, 11732, 11732, 6340, 5680, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688]\n"
     ]
    }
   ],
   "source": [
    "# Define the Seq2Seq model with GRU\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder component of the Seq2Seq model using GRU\n",
    "    Processes the input sequence and generates context vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size=512, hidden_size=1024, num_layers=2, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer to convert token IDs to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        \n",
    "        # GRU layer for processing sequences\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, \n",
    "                         batch_first=True, dropout=dropout, bidirectional=False)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder\n",
    "        \n",
    "        Args:\n",
    "            input_seq: Input token sequences [batch_size, seq_len]\n",
    "            input_lengths: Actual lengths of sequences (for packed sequences)\n",
    "            \n",
    "        Returns:\n",
    "            outputs: All hidden states [batch_size, seq_len, hidden_size]\n",
    "            hidden: Final hidden state [num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert token IDs to embeddings\n",
    "        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embed_size]\n",
    "        \n",
    "        # Pass through GRU\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # outputs: [batch_size, seq_len, hidden_size]\n",
    "        # hidden: [num_layers, batch_size, hidden_size] \n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder component of the Seq2Seq model using GRU\n",
    "    Generates output sequence one token at a time\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size=512, hidden_size=1024, num_layers=2, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer for target tokens\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        \n",
    "        # GRU layer for generating sequences\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, \n",
    "                         batch_first=True, dropout=dropout, bidirectional=False)\n",
    "        \n",
    "        # Output projection layer to vocabulary\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input_token, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder (single step)\n",
    "        \n",
    "        Args:\n",
    "            input_token: Current input token [batch_size, 1]\n",
    "            hidden: Hidden state from encoder/previous step [num_layers, batch_size, hidden_size]\n",
    "            \n",
    "        Returns:\n",
    "            output: Vocabulary predictions [batch_size, vocab_size]\n",
    "            hidden: Updated hidden state [num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # input_token: [batch_size, 1]\n",
    "        embedded = self.embedding(input_token)  # [batch_size, 1, embed_size]\n",
    "        \n",
    "        # Pass through GRU\n",
    "        gru_out, hidden = self.rnn(embedded, hidden)\n",
    "        # gru_out: [batch_size, 1, hidden_size]\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.output_projection(gru_out.squeeze(1))  # [batch_size, vocab_size]\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Sequence-to-Sequence model using GRU\n",
    "    Combines Encoder and Decoder for translation\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size, embed_size=512, \n",
    "                 hidden_size=1024, num_layers=2, dropout=0.1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initialize encoder and decoder\n",
    "        self.encoder = Encoder(encoder_vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = Decoder(decoder_vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "        \n",
    "    def forward(self, source_seq, target_seq):\n",
    "        \"\"\"\n",
    "        Forward pass of the complete Seq2Seq model\n",
    "        \n",
    "        Args:\n",
    "            source_seq: Source language sequences [batch_size, source_len]\n",
    "            target_seq: Target language sequences [batch_size, target_len]\n",
    "            teacher_forcing_ratio: Probability of using teacher forcing\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Predicted sequences [batch_size, target_len, vocab_size]\n",
    "        \"\"\"\n",
    "        batch_size = source_seq.size(0)\n",
    "        target_len = target_seq.size(1)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        _, hidden = self.encoder(source_seq)\n",
    "        \n",
    "        # Store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, target_len, self.decoder_vocab_size).to(source_seq.device)\n",
    "        \n",
    "        # Decode step by step\n",
    "        for t in range(target_len):\n",
    "            # Get decoder output for current step\n",
    "            decoder_input = target_seq[:, t].unsqueeze(1)\n",
    "            output, hidden = self.decoder(decoder_input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, source_seq, max_length=100, start_token=101, end_token=102):\n",
    "        \"\"\"\n",
    "        Generate translation for given source sequence (inference mode)\n",
    "        \n",
    "        Args:\n",
    "            source_seq: Source sequence [batch_size, source_len]\n",
    "            max_length: Maximum length of generated sequence\n",
    "            start_token: BOS token ID (101 for BERT)\n",
    "            end_token: EOS token ID (102 for BERT)\n",
    "            \n",
    "        Returns:\n",
    "            generated_seq: Generated sequence [batch_size, generated_len]\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        batch_size = source_seq.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode source sequence\n",
    "            _, hidden = self.encoder(source_seq)\n",
    "            \n",
    "            # Initialize with start token\n",
    "            decoder_input = torch.full((batch_size, 1), start_token, dtype=torch.long).to(source_seq.device)\n",
    "            \n",
    "            # Store generated tokens\n",
    "            generated_tokens = []\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                # Get next token prediction\n",
    "                output, hidden = self.decoder(decoder_input, hidden)\n",
    "                \n",
    "                # Get the token with highest probability\n",
    "                next_token = output.argmax(dim=1).unsqueeze(1)\n",
    "                generated_tokens.append(next_token)\n",
    "                \n",
    "                # Use predicted token as next input\n",
    "                decoder_input = next_token\n",
    "                \n",
    "                # Stop if all sequences generated EOS token\n",
    "                if torch.all(next_token.squeeze() == end_token):\n",
    "                    break\n",
    "            \n",
    "            # Concatenate all generated tokens\n",
    "            generated_seq = torch.cat(generated_tokens, dim=1)\n",
    "            \n",
    "        return generated_seq\n",
    "\n",
    "# Model configuration based on your dataset\n",
    "model_config = {\n",
    "    'encoder_vocab_size': encoder_vocab_size,  # 30522 (English BERT)\n",
    "    'decoder_vocab_size': decoder_vocab_size,  # 21128 (Chinese BERT)\n",
    "    'embed_size': 256,\n",
    "    'hidden_size': 512,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = Seq2Seq(**model_config)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"=== Seq2Seq Model with GRU Architecture ===\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Encoder Vocabulary Size: {model_config['encoder_vocab_size']:,}\")\n",
    "print(f\"Decoder Vocabulary Size: {model_config['decoder_vocab_size']:,}\")\n",
    "print(f\"Embedding Size: {model_config['embed_size']}\")\n",
    "print(f\"Hidden Size: {model_config['hidden_size']}\")\n",
    "print(f\"Number of Layers: {model_config['num_layers']}\")\n",
    "print(f\"Dropout Rate: {model_config['dropout']}\")\n",
    "print(f\"RNN Type: GRU (Gated Recurrent Unit)\")\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Model Size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"\\n=== Model Architecture ===\")\n",
    "print(model)\n",
    "\n",
    "# Test the model with a sample batch\n",
    "print(f\"\\n=== Testing Model Forward Pass ===\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "source_ids = sample_batch['source_ids'].to(device)\n",
    "target_ids = sample_batch['target_ids'].to(device)\n",
    "\n",
    "print(f\"Source shape: {source_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.train()\n",
    "outputs = model(source_ids, target_ids)\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "print(f\"Expected shape: [batch_size, target_len, decoder_vocab_size]\")\n",
    "print(f\"Actual shape: [{outputs.shape[0]}, {outputs.shape[1]}, {outputs.shape[2]}]\")\n",
    "\n",
    "# Test generation\n",
    "print(f\"\\n=== Testing Model Generation ===\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(source_ids[:2], max_length=100)  # Generate for first 2 samples\n",
    "    print(f\"Generated sequence shape: {generated.shape}\")\n",
    "    print(f\"Generated tokens (first sample): {generated[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "=== Starting Training for 10 Epochs ===\n",
      "Device: cpu\n",
      "Train batches: 33899\n",
      "Validation batches: 1785\n",
      "Model parameters: 29,578,376\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/33899 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 264\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Start training for 10 epochs\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_seq2seq_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlotting training history...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 153\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, num_epochs, save_path)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m    156\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_epoch()\n",
      "Cell \u001b[0;32mIn[11], line 69\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m target_ids[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Remove last token\u001b[39;00m\n\u001b[1;32m     67\u001b[0m decoder_targets \u001b[38;5;241m=\u001b[39m target_ids[:, \u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Remove first token (BOS)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_loss(outputs, decoder_targets)\n",
      "File \u001b[0;32m~/archer/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/archer/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 142\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, source_seq, target_seq)\u001b[0m\n\u001b[1;32m    140\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m target_seq[:, t]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    141\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(decoder_input, hidden)\n\u001b[0;32m--> 142\u001b[0m     outputs[:, t, :] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model with comprehensive training loop\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for Seq2Seq machine translation model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_dataloader, val_dataloader, device, \n",
    "                 learning_rate=1e-3, weight_decay=1e-5):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.device = device\n",
    "        \n",
    "        # Loss function - ignore padding tokens (index 0)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(model.parameters(), \n",
    "                                  lr=learning_rate, \n",
    "                                  weight_decay=weight_decay)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = defaultdict(list)\n",
    "        \n",
    "    def calculate_loss(self, outputs, targets, target_lengths=None):\n",
    "        \"\"\"Calculate loss for the batch\"\"\"\n",
    "        # Reshape for loss calculation (use reshape instead of view for non-contiguous tensors)\n",
    "        outputs_flat = outputs.reshape(-1, outputs.size(-1))\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.criterion(outputs_flat, targets_flat)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(self.train_dataloader, desc=\"Training\", leave=False)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            source_ids = batch['source_ids'].to(self.device)\n",
    "            target_ids = batch['target_ids'].to(self.device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with teacher forcing\n",
    "            decoder_input = target_ids[:, :-1]  # Remove last token\n",
    "            decoder_targets = target_ids[:, 1:]  # Remove first token (BOS)\n",
    "            \n",
    "            outputs = self.model(source_ids, decoder_input)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.calculate_loss(outputs, decoder_targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{epoch_loss / num_batches:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if batch_idx % 100 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return epoch_loss / num_batches\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(self.val_dataloader, desc=\"Validation\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                source_ids = batch['source_ids'].to(self.device)\n",
    "                target_ids = batch['target_ids'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                decoder_input = target_ids[:, :-1]\n",
    "                decoder_targets = target_ids[:, 1:]\n",
    "                \n",
    "                outputs = self.model(source_ids, decoder_input)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = self.calculate_loss(outputs, decoder_targets)\n",
    "                \n",
    "                # Update metrics\n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'Val Loss': f'{loss.item():.4f}',\n",
    "                    'Avg Val Loss': f'{epoch_loss / num_batches:.4f}'\n",
    "                })\n",
    "        \n",
    "        return epoch_loss / num_batches\n",
    "    \n",
    "    def train(self, num_epochs=10, save_path=None):\n",
    "        \"\"\"Train the model for specified number of epochs\"\"\"\n",
    "        print(f\"=== Starting Training for {num_epochs} Epochs ===\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Train batches: {len(self.train_dataloader)}\")\n",
    "        print(f\"Validation batches: {len(self.val_dataloader)}\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss = self.validate_epoch()\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Calculate epoch time\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            \n",
    "            # Store history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['epoch_time'].append(epoch_time)\n",
    "            self.history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                print(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "                \n",
    "                if save_path:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'model_config': model_config\n",
    "                    }, save_path)\n",
    "                    print(f\"Model saved to {save_path}\")\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Training summary\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training completed in {total_time:.2f}s ({total_time/60:.2f} min)\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "        print(f\"Final train loss: {self.history['train_loss'][-1]:.4f}\")\n",
    "        print(f\"Final val loss: {self.history['val_loss'][-1]:.4f}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if not self.history['train_loss']:\n",
    "            print(\"No training history to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[0, 0].plot(self.history['train_loss'], label='Train Loss', color='blue')\n",
    "        axes[0, 0].plot(self.history['val_loss'], label='Validation Loss', color='red')\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot learning rate\n",
    "        axes[0, 1].plot(self.history['learning_rate'], color='green')\n",
    "        axes[0, 1].set_title('Learning Rate Schedule')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Learning Rate')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot epoch time\n",
    "        axes[1, 0].plot(self.history['epoch_time'], color='orange')\n",
    "        axes[1, 0].set_title('Epoch Training Time')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Time (seconds)')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot loss comparison\n",
    "        x = range(len(self.history['train_loss']))\n",
    "        axes[1, 1].fill_between(x, self.history['train_loss'], alpha=0.3, color='blue', label='Train Loss')\n",
    "        axes[1, 1].fill_between(x, self.history['val_loss'], alpha=0.3, color='red', label='Val Loss')\n",
    "        axes[1, 1].plot(self.history['train_loss'], color='blue', linewidth=2)\n",
    "        axes[1, 1].plot(self.history['val_loss'], color='red', linewidth=2)\n",
    "        axes[1, 1].set_title('Loss Comparison')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=device,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Start training for 10 epochs\n",
    "print(\"Starting training...\")\n",
    "history = trainer.train(num_epochs=10, save_path=\"best_seq2seq_model.pth\")\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nPlotting training history...\")\n",
    "trainer.plot_training_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Machine Translation Testing Code\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "class TranslationTester:\n",
    "    \"\"\"\n",
    "    Comprehensive tester for the Seq2Seq machine translation model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device, max_length=100):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load tokenizers\n",
    "        self.tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "        \n",
    "        # Special tokens\n",
    "        self.start_token = 101  # [BOS] token\n",
    "        self.end_token = 102   # [EOS] token\n",
    "        self.pad_token = 0     # [PAD] token\n",
    "        \n",
    "        print(\"=== Translation Tester Initialized ===\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"English tokenizer vocab size: {self.tokenizer_en.vocab_size}\")\n",
    "        print(f\"Chinese tokenizer vocab size: {self.tokenizer_zh.vocab_size}\")\n",
    "        print(f\"Max sequence length: {max_length}\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess input text\"\"\"\n",
    "        # Basic cleaning\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single space\n",
    "        return text\n",
    "\n",
    "    def encode_english(self, text):\n",
    "        \"\"\"Encode English text to token IDs\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        tokens = self.tokenizer_en(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens['input_ids'].to(self.device)\n",
    "\n",
    "    def decode_chinese(self, token_ids):\n",
    "        \"\"\"Decode Chinese token IDs to text\"\"\"\n",
    "        # Remove padding tokens and special tokens\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.cpu().tolist()\n",
    "        \n",
    "        # Remove padding (0), start (101), and end (102) tokens\n",
    "        cleaned_tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in [0, 101, 102]:\n",
    "                cleaned_tokens.append(token_id)\n",
    "            elif token_id == 102:  # Stop at end token\n",
    "                break\n",
    "        \n",
    "        if not cleaned_tokens:\n",
    "            return \"[Empty translation]\"\n",
    "        \n",
    "        try:\n",
    "            text = self.tokenizer_zh.decode(cleaned_tokens, skip_special_tokens=True)\n",
    "            # Clean up extra spaces\n",
    "            text = re.sub(r'\\s+', '', text)  # Remove all spaces for Chinese\n",
    "            return text if text else \"[Empty translation]\"\n",
    "        except Exception as e:\n",
    "            return f\"[Decoding error: {str(e)}]\"\n",
    "\n",
    "    def translate_single(self, english_text, beam_size=1, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Translate a single English sentence to Chinese\n",
    "        \n",
    "        Args:\n",
    "            english_text: Input English text\n",
    "            beam_size: Beam search size (1 for greedy)\n",
    "            temperature: Sampling temperature for generation\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode input\n",
    "            source_ids = self.encode_english(english_text)\n",
    "            \n",
    "            if beam_size == 1:\n",
    "                # Greedy decoding\n",
    "                generated = self.model.generate(\n",
    "                    source_ids, \n",
    "                    max_length=self.max_length,\n",
    "                    start_token=self.start_token,\n",
    "                    end_token=self.end_token\n",
    "                )\n",
    "                translation = self.decode_chinese(generated[0])\n",
    "            else:\n",
    "                # Beam search (simplified version)\n",
    "                translation = self._beam_search_translate(source_ids, beam_size, temperature)\n",
    "        \n",
    "        return translation\n",
    "\n",
    "    def _beam_search_translate(self, source_ids, beam_size=3, temperature=1.0):\n",
    "        \"\"\"Simplified beam search implementation\"\"\"\n",
    "        batch_size = source_ids.size(0)\n",
    "        \n",
    "        # Encode source\n",
    "        _, hidden = self.model.encoder(source_ids)\n",
    "        \n",
    "        # Initialize beams\n",
    "        beams = [(torch.tensor([[self.start_token]]).to(self.device), 0.0, hidden)]\n",
    "        \n",
    "        for step in range(self.max_length):\n",
    "            new_beams = []\n",
    "            \n",
    "            for seq, score, h in beams:\n",
    "                if seq[0, -1].item() == self.end_token:\n",
    "                    new_beams.append((seq, score, h))\n",
    "                    continue\n",
    "                \n",
    "                # Get next token probabilities\n",
    "                last_token = seq[:, -1:] \n",
    "                output, new_h = self.model.decoder(last_token, h)\n",
    "                \n",
    "                # Apply temperature\n",
    "                if temperature != 1.0:\n",
    "                    output = output / temperature\n",
    "                \n",
    "                probs = F.softmax(output, dim=-1)\n",
    "                \n",
    "                # Get top k candidates\n",
    "                top_probs, top_indices = torch.topk(probs, beam_size)\n",
    "                \n",
    "                for i in range(beam_size):\n",
    "                    token_id = top_indices[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                    token_prob = top_probs[0, i].item()\n",
    "                    new_seq = torch.cat([seq, token_id], dim=1)\n",
    "                    new_score = score + math.log(token_prob + 1e-10)\n",
    "                    new_beams.append((new_seq, new_score, new_h))\n",
    "            \n",
    "            # Keep only top beams\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "            \n",
    "            # Check if all beams ended\n",
    "            if all(seq[0, -1].item() == self.end_token for seq, _, _ in beams):\n",
    "                break\n",
    "        \n",
    "        # Return best beam\n",
    "        best_seq = beams[0][0]\n",
    "        return self.decode_chinese(best_seq[0])\n",
    "\n",
    "    def calculate_bleu_score(self, reference, candidate):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score (simplified version)\n",
    "        \"\"\"\n",
    "        def get_ngrams(tokens, n):\n",
    "            return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "        \n",
    "        # Tokenize (character-level for Chinese)\n",
    "        ref_tokens = list(reference.replace(' ', ''))\n",
    "        cand_tokens = list(candidate.replace(' ', ''))\n",
    "        \n",
    "        if len(cand_tokens) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate precision for n-grams (1 to 4)\n",
    "        precisions = []\n",
    "        for n in range(1, 5):\n",
    "            ref_ngrams = Counter(get_ngrams(ref_tokens, n))\n",
    "            cand_ngrams = Counter(get_ngrams(cand_tokens, n))\n",
    "            \n",
    "            if len(cand_ngrams) == 0:\n",
    "                precisions.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            matches = sum((ref_ngrams & cand_ngrams).values())\n",
    "            precision = matches / len(get_ngrams(cand_tokens, n))\n",
    "            precisions.append(precision)\n",
    "        \n",
    "        # Calculate brevity penalty\n",
    "        ref_len = len(ref_tokens)\n",
    "        cand_len = len(cand_tokens)\n",
    "        \n",
    "        if cand_len > ref_len:\n",
    "            bp = 1.0\n",
    "        else:\n",
    "            bp = math.exp(1 - ref_len / (cand_len + 1e-10))\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        if min(precisions) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        bleu = bp * math.exp(sum(math.log(p + 1e-10) for p in precisions) / 4)\n",
    "        return bleu\n",
    "\n",
    "    def test_examples(self, test_cases, beam_size=1, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Test the model on multiple examples\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MACHINE TRANSLATION TESTING RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Model: Seq2Seq with GRU\")\n",
    "        print(f\"Beam size: {beam_size}\")\n",
    "        print(f\"Temperature: {temperature}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        results = []\n",
    "        total_time = 0\n",
    "        \n",
    "        for i, (english, expected_chinese) in enumerate(test_cases, 1):\n",
    "            print(f\"\\nTest {i}/{len(test_cases)}\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"English:  {english}\")\n",
    "            \n",
    "            # Translate\n",
    "            start_time = time.time()\n",
    "            translation = self.translate_single(english, beam_size, temperature)\n",
    "            translation_time = time.time() - start_time\n",
    "            total_time += translation_time\n",
    "            \n",
    "            print(f\"Expected: {expected_chinese}\")\n",
    "            print(f\"Generated: {translation}\")\n",
    "            print(f\"Time: {translation_time:.3f}s\")\n",
    "            \n",
    "            # Calculate BLEU score if reference is provided\n",
    "            if expected_chinese and expected_chinese != \"\":\n",
    "                bleu_score = self.calculate_bleu_score(expected_chinese, translation)\n",
    "                print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "            else:\n",
    "                bleu_score = None\n",
    "                print(\"BLEU Score: N/A (no reference)\")\n",
    "            \n",
    "            results.append({\n",
    "                'english': english,\n",
    "                'expected': expected_chinese,\n",
    "                'translation': translation,\n",
    "                'time': translation_time,\n",
    "                'bleu': bleu_score\n",
    "            })\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total examples: {len(test_cases)}\")\n",
    "        print(f\"Total time: {total_time:.3f}s\")\n",
    "        print(f\"Average time per translation: {total_time/len(test_cases):.3f}s\")\n",
    "        \n",
    "        bleu_scores = [r['bleu'] for r in results if r['bleu'] is not None]\n",
    "        if bleu_scores:\n",
    "            print(f\"Average BLEU score: {np.mean(bleu_scores):.4f}\")\n",
    "            print(f\"Max BLEU score: {max(bleu_scores):.4f}\")\n",
    "            print(f\"Min BLEU score: {min(bleu_scores):.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def interactive_translation(self):\n",
    "        \"\"\"Interactive translation mode\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INTERACTIVE TRANSLATION MODE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Enter English text to translate (type 'quit' to exit)\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                english_text = input(\"\\nEnglish: \").strip()\n",
    "                \n",
    "                if english_text.lower() in ['quit', 'exit', 'q']:\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if not english_text:\n",
    "                    print(\"Please enter some text.\")\n",
    "                    continue\n",
    "                \n",
    "                # Translate\n",
    "                start_time = time.time()\n",
    "                translation = self.translate_single(english_text)\n",
    "                translation_time = time.time() - start_time\n",
    "                \n",
    "                print(f\"Chinese:  {translation}\")\n",
    "                print(f\"Time: {translation_time:.3f}s\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nGoodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Real-world test cases for English to Chinese translation\n",
    "REAL_WORLD_TEST_CASES = [\n",
    "    # Basic greetings and common phrases\n",
    "    (\"Hello, how are you?\", \"你好，你好吗？\"),\n",
    "    (\"Good morning!\", \"早上好！\"),\n",
    "    (\"Thank you very much.\", \"非常感谢。\"),\n",
    "    (\"Nice to meet you.\", \"很高兴见到你。\"),\n",
    "    (\"See you later.\", \"再见。\"),\n",
    "    \n",
    "    # Simple sentences\n",
    "    (\"I love you.\", \"我爱你。\"),\n",
    "    (\"What is your name?\", \"你叫什么名字？\"),\n",
    "    (\"Where are you from?\", \"你来自哪里？\"),\n",
    "    (\"How much does this cost?\", \"这个多少钱？\"),\n",
    "    (\"Can you help me?\", \"你能帮助我吗？\"),\n",
    "    \n",
    "    # Daily life\n",
    "    (\"I am hungry.\", \"我饿了。\"),\n",
    "    (\"The weather is nice today.\", \"今天天气很好。\"),\n",
    "    (\"I need to go to work.\", \"我需要去工作。\"),\n",
    "    (\"Let's have dinner together.\", \"我们一起吃晚饭吧。\"),\n",
    "    (\"I am learning Chinese.\", \"我在学中文。\"),\n",
    "    \n",
    "    # Travel and directions\n",
    "    (\"Where is the hotel?\", \"酒店在哪里？\"),\n",
    "    (\"I want to go to the airport.\", \"我想去机场。\"),\n",
    "    (\"How do I get to the train station?\", \"我怎么去火车站？\"),\n",
    "    (\"Is this the right way?\", \"这是正确的路吗？\"),\n",
    "    (\"I am lost.\", \"我迷路了。\"),\n",
    "    \n",
    "    # Business and technology\n",
    "    (\"I work in technology.\", \"我在科技行业工作。\"),\n",
    "    (\"The meeting is at 3 PM.\", \"会议在下午3点。\"),\n",
    "    (\"Please send me the document.\", \"请发给我文件。\"),\n",
    "    (\"This is a good opportunity.\", \"这是一个好机会。\"),\n",
    "    (\"We need to discuss this further.\", \"我们需要进一步讨论这个。\"),\n",
    "    \n",
    "    # Complex sentences\n",
    "    (\"Although it was raining, we decided to go out.\", \"虽然下雨了，我们还是决定出去。\"),\n",
    "    (\"The book that I read yesterday was very interesting.\", \"我昨天读的那本书很有趣。\"),\n",
    "    (\"If you study hard, you will succeed.\", \"如果你努力学习，你会成功的。\"),\n",
    "    (\"She said she would come, but she didn't show up.\", \"她说她会来，但她没有出现。\"),\n",
    "    (\"The restaurant we went to last night had excellent food.\", \"我们昨晚去的餐厅食物很棒。\"),\n",
    "    \n",
    "    # Without reference translations (for testing only)\n",
    "    (\"Machine learning is revolutionizing many industries.\", \"\"),\n",
    "    (\"Climate change is one of the biggest challenges of our time.\", \"\"),\n",
    "    (\"The internet has connected people around the world.\", \"\"),\n",
    "    (\"Artificial intelligence will shape the future of humanity.\", \"\"),\n",
    "    (\"Education is the key to unlocking human potential.\", \"\"),\n",
    "]\n",
    "\n",
    "# Initialize the tester\n",
    "print(\"Initializing translation tester...\")\n",
    "tester = TranslationTester(model, device, max_length=100)\n",
    "\n",
    "# Test with real-world examples\n",
    "print(\"\\nTesting with real-world examples...\")\n",
    "results = tester.test_examples(REAL_WORLD_TEST_CASES[:10], beam_size=1, temperature=1.0)\n",
    "\n",
    "# Test with beam search (more advanced decoding)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING WITH BEAM SEARCH (Beam size = 3)\")\n",
    "print(\"=\"*80)\n",
    "beam_results = tester.test_examples(REAL_WORLD_TEST_CASES[10:15], beam_size=3, temperature=0.8)\n",
    "\n",
    "# Compare different decoding strategies\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARING DECODING STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_sentence = \"I love learning new languages.\"\n",
    "expected = \"我喜欢学习新语言。\"\n",
    "\n",
    "print(f\"Test sentence: {test_sentence}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Greedy decoding\n",
    "greedy_translation = tester.translate_single(test_sentence, beam_size=1, temperature=1.0)\n",
    "greedy_bleu = tester.calculate_bleu_score(expected, greedy_translation)\n",
    "print(f\"Greedy:       {greedy_translation} (BLEU: {greedy_bleu:.4f})\")\n",
    "\n",
    "# Beam search\n",
    "beam_translation = tester.translate_single(test_sentence, beam_size=3, temperature=0.8)\n",
    "beam_bleu = tester.calculate_bleu_score(expected, beam_translation)\n",
    "print(f\"Beam Search:  {beam_translation} (BLEU: {beam_bleu:.4f})\")\n",
    "\n",
    "# High temperature (more random)\n",
    "random_translation = tester.translate_single(test_sentence, beam_size=1, temperature=1.5)\n",
    "random_bleu = tester.calculate_bleu_score(expected, random_translation)\n",
    "print(f\"High Temp:    {random_translation} (BLEU: {random_bleu:.4f})\")\n",
    "\n",
    "# Performance analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test translation speed with different sentence lengths\n",
    "speed_test_cases = [\n",
    "    \"Hello.\",  # Short\n",
    "    \"How are you doing today?\",  # Medium\n",
    "    \"I really enjoy reading books and learning about different cultures around the world.\",  # Long\n",
    "    \"The quick brown fox jumps over the lazy dog while the sun is shining brightly in the clear blue sky.\"  # Very long\n",
    "]\n",
    "\n",
    "print(\"Testing translation speed with different sentence lengths:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, sentence in enumerate(speed_test_cases, 1):\n",
    "    start_time = time.time()\n",
    "    translation = tester.translate_single(sentence)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Length {i} ({len(sentence.split())} words): {end_time - start_time:.3f}s\")\n",
    "    print(f\"  Input:  {sentence}\")\n",
    "    print(f\"  Output: {translation}\")\n",
    "    print()\n",
    "\n",
    "# Start interactive mode (optional)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Would you like to try interactive translation? (y/n)\")\n",
    "response = input().strip().lower()\n",
    "\n",
    "if response in ['y', 'yes']:\n",
    "    tester.interactive_translation()\n",
    "else:\n",
    "    print(\"Testing completed! You can run `tester.interactive_translation()` anytime for manual testing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
